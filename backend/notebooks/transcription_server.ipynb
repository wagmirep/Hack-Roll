{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ö†Ô∏è RUN THIS CELL TO RESET RUNTIME (only if you have dependency issues)\n",
    "# This disconnects and deletes the runtime - you'll need to reconnect after\n",
    "from google.colab import runtime\n",
    "runtime.unassign()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LahStats - ML Server (Transcription + Diarization)\n",
    "\n",
    "Runs MERaLiON (transcription) and pyannote (speaker diarization) as a persistent API server.\n",
    "\n",
    "**Setup:**\n",
    "1. Run **Cell 2** (Install dependencies) - uses Colab's pre-installed torch\n",
    "2. **Restart Runtime** (Runtime ‚Üí Restart runtime)\n",
    "3. Run **Cell 3** onwards (skip Cell 1 & 2 after restart)\n",
    "4. Set your tokens in **Cell 4** (ngrok + HuggingFace)\n",
    "5. Copy the ngrok URL to your backend .env\n",
    "6. Keep this notebook running during the demo\n",
    "\n",
    "**‚ö†Ô∏è Important:** Do NOT uninstall Colab's pre-installed torch/torchvision - it causes version mismatch errors!\n",
    "\n",
    "**Endpoints:**\n",
    "- `GET /health` - Check server status\n",
    "- `POST /transcribe` - Transcribe audio\n",
    "- `POST /diarize` - Speaker diarization (who spoke when)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking pre-installed PyTorch versions...\n",
      "Name: torch\n",
      "Version: 2.9.1\n",
      "Name: torchaudio\n",
      "Version: 2.9.1\n",
      "\n",
      "Installing pyannote.audio and dependencies...\n",
      "\n",
      "Installing transformers and other dependencies...\n",
      "\n",
      "============================================================\n",
      "‚úÖ Dependencies installed!\n",
      "‚ö†Ô∏è  NOW RESTART RUNTIME:\n",
      "   Runtime ‚Üí Restart runtime\n",
      "   Then run Cell 3 onwards (skip this cell)\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Install dependencies - LOCK Colab's torch versions to prevent downgrades!\n",
    "# pyannote.audio dependencies try to downgrade torch, breaking torchvision.\n",
    "\n",
    "import subprocess\n",
    "\n",
    "# Step 1: Get current torch versions and create constraints file\n",
    "print(\"Step 1: Locking PyTorch versions...\")\n",
    "result = subprocess.run(['pip', 'show', 'torch'], capture_output=True, text=True)\n",
    "torch_version = [line.split(': ')[1] for line in result.stdout.split('\\n') if line.startswith('Version:')][0]\n",
    "print(f\"  torch=={torch_version}\")\n",
    "\n",
    "result = subprocess.run(['pip', 'show', 'torchvision'], capture_output=True, text=True)\n",
    "torchvision_version = [line.split(': ')[1] for line in result.stdout.split('\\n') if line.startswith('Version:')][0]\n",
    "print(f\"  torchvision=={torchvision_version}\")\n",
    "\n",
    "result = subprocess.run(['pip', 'show', 'torchaudio'], capture_output=True, text=True)\n",
    "torchaudio_version = [line.split(': ')[1] for line in result.stdout.split('\\n') if line.startswith('Version:')][0]\n",
    "print(f\"  torchaudio=={torchaudio_version}\")\n",
    "\n",
    "# Create constraints file to prevent torch downgrades\n",
    "with open('/tmp/torch_constraints.txt', 'w') as f:\n",
    "    f.write(f\"torch=={torch_version}\\n\")\n",
    "    f.write(f\"torchvision=={torchvision_version}\\n\")\n",
    "    f.write(f\"torchaudio=={torchaudio_version}\\n\")\n",
    "print(\"  ‚úÖ Constraints file created\")\n",
    "\n",
    "# Step 2: Install pyannote.audio with constraints (prevents torch downgrade)\n",
    "print(\"\\nStep 2: Installing pyannote.audio (with torch locked)...\")\n",
    "!pip install -q pyannote.audio -c /tmp/torch_constraints.txt 2>&1 | tail -5\n",
    "\n",
    "# Step 3: Install transformers and other deps\n",
    "print(\"\\nStep 3: Installing transformers and other dependencies...\")\n",
    "!pip install -q \"transformers>=4.40.0,<4.50.0\" accelerate librosa flask pyngrok -c /tmp/torch_constraints.txt 2>&1 | tail -3\n",
    "\n",
    "# Step 4: Verify torch versions weren't changed\n",
    "print(\"\\nStep 4: Verifying torch versions...\")\n",
    "!pip show torch torchvision 2>/dev/null | grep -E \"^(Name|Version):\"\n",
    "\n",
    "# Step 5: Test pyannote import\n",
    "print(\"\\nStep 5: Testing pyannote import...\")\n",
    "!python -c \"from pyannote.audio import Pipeline; print('‚úÖ pyannote.audio imported successfully')\" 2>&1 || echo \"‚ùå pyannote import failed - check errors above\"\n",
    "\n",
    "# Restart message\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ Dependencies installed!\")\n",
    "print(\"‚ö†Ô∏è  NOW RESTART RUNTIME:\")\n",
    "print(\"   Runtime ‚Üí Restart runtime\")\n",
    "print(\"   Then run Cell 3 onwards (skip this cell)\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "GPU: NVIDIA H100 80GB HBM3\n",
      "Memory: 85.2 GB\n",
      "Torch version: 2.9.1+cu128\n"
     ]
    }
   ],
   "source": [
    "# Check GPU availability (run after restart)\n",
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    print(f\"Torch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NGROK_AUTHTOKEN: ‚úÖ Set\n",
      "HUGGINGFACE_TOKEN: ‚úÖ Set\n"
     ]
    }
   ],
   "source": [
    "# Set tokens - paste your tokens here (this notebook is gitignored)\n",
    "import os\n",
    "\n",
    "# Ngrok token for public URL\n",
    "os.environ[\"NGROK_AUTHTOKEN\"] = \"YOUR_NGROK_TOKEN_HERE\"  # <-- Paste your ngrok token here\n",
    "\n",
    "# HuggingFace token for pyannote (get from https://huggingface.co/settings/tokens)\n",
    "# Also accept pyannote license at https://huggingface.co/pyannote/speaker-diarization-3.1\n",
    "os.environ[\"HUGGINGFACE_TOKEN\"] = \"YOUR_HUGGINGFACE_TOKEN_HERE\"  # <-- Paste your HF token here\n",
    "\n",
    "print(\"NGROK_AUTHTOKEN:\", \"‚úÖ Set\" if os.environ.get(\"NGROK_AUTHTOKEN\") else \"‚ùå Missing\")\n",
    "print(\"HUGGINGFACE_TOKEN:\", \"‚úÖ Set\" if os.environ.get(\"HUGGINGFACE_TOKEN\") else \"‚ùå Missing (needed for diarization)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading processor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
      "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
      "You are not authenticated with the Hugging Face Hub in this notebook.\n",
      "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2d0da6290aa4b3f8f853feeb456d04e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "processor_config.json:   0%|          | 0.00/281 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba563df72b8a42d391cb9094140b0096",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "processing_meralion2.py: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/MERaLiON/MERaLiON-2-3B:\n",
      "- processing_meralion2.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf8781fd9009412eaeff116645d3272e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/443 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f887fa982b35483d8670b8da3c4fdbb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f581d5ccaa184f73851374618d3d1e30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a37cabac2934fc0a9ffafc39d3a14d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/34.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d71553da3534e8e80d88bb451fa79be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model (this may take a few minutes)...\n",
      "Using model: MERaLiON/MERaLiON-2-3B\n",
      "GPU Memory: 85.2 GB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5115d6209ec044c581846acec276bf8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c8920ed0a1842a5a477a1a78c27862a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "configuration_meralion2.py: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/MERaLiON/MERaLiON-2-3B:\n",
      "- configuration_meralion2.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "801447deb2fe46b5b3c1660cca23ceb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modeling_meralion2.py: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/MERaLiON/MERaLiON-2-3B:\n",
      "- modeling_meralion2.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "261ed5a9ff1c410aa7018098873a280e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b159888899e46c089763a4ed6e68ee9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27954fef56054a43ba12da186697c8fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0504920bae9a43bfb6bd9b60a769fa2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/1.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3790f6c56e0942d48032bf14e345d3d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "169afc49e97f46ac99d668ba2829df78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/197 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on GPU (float16)\n",
      "Model loaded on cuda:0!\n"
     ]
    }
   ],
   "source": [
    "# Load MERaLiON-2-3B-ASR model (smaller, fits on most GPUs)\n",
    "# Use 3B instead of 10B to avoid OOM errors - matches backend service\n",
    "from transformers import AutoProcessor, AutoModelForSpeechSeq2Seq\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "# Clear CUDA cache before loading\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "MODEL_NAME = \"MERaLiON/MERaLiON-2-3B\"  # Changed from 10B to 3B to avoid OOM\n",
    "\n",
    "print(\"Loading processor...\")\n",
    "processor = AutoProcessor.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "\n",
    "print(\"Loading model (this may take a few minutes)...\")\n",
    "print(f\"Using model: {MODEL_NAME}\")\n",
    "\n",
    "# Check GPU memory\n",
    "if torch.cuda.is_available():\n",
    "    total_mem = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"GPU Memory: {total_mem:.1f} GB\")\n",
    "    \n",
    "    # Load with float16 for GPU\n",
    "    model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        torch_dtype=torch.float16,\n",
    "        trust_remote_code=True,\n",
    "        attn_implementation=\"eager\",\n",
    "        low_cpu_mem_usage=True,\n",
    "        device_map=\"auto\",  # Automatic device placement\n",
    "    )\n",
    "    print(f\"Model loaded on GPU (float16)\")\n",
    "else:\n",
    "    # CPU fallback\n",
    "    print(\"No GPU available, loading for CPU...\")\n",
    "    model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        torch_dtype=torch.float32,\n",
    "        trust_remote_code=True,\n",
    "        attn_implementation=\"eager\",\n",
    "    )\n",
    "    print(\"Model loaded on CPU\")\n",
    "\n",
    "model.eval()  # Set to evaluation mode\n",
    "\n",
    "print(f\"Model loaded on {next(model.parameters()).device}!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch_audiomentations'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-2120749973.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load pyannote speaker diarization model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpyannote\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maudio\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mHF_TOKEN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"HUGGINGFACE_TOKEN\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"hf_iuiyPxwdQoNqAnqcoFDdciVb1t1FTbOtBL\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyannote/audio/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0m__version__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"pyannote-audio\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minference\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAudio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyannote/audio/core/inference.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlightning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpytorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutilities\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mis_oom_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyannote\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAudioFile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpyannote\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSpecifications\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyannote\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mResolution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyannote\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmulti_task\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmap_with_specifications\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyannote/audio/core/model.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyannote\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maudio\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyannote\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAudio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m from pyannote.audio.core.task import (\n\u001b[0m\u001b[1;32m     44\u001b[0m     \u001b[0mProblem\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mSpecifications\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyannote/audio/core/task.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyannote\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatabase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mScope\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSubset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIterableDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch_audiomentations\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mIdentity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch_audiomentations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms_interface\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaseWaveformTransform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMetricCollection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch_audiomentations'",
      "",
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Load pyannote speaker diarization model\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.serialization\n",
    "import importlib\n",
    "\n",
    "# PyTorch 2.6+ fix: pyannote checkpoints aren't compatible with weights_only=True\n",
    "# We MUST patch before pyannote imports torch.load\n",
    "\n",
    "# Step 1: Remove any cached pyannote imports so they pick up our patch\n",
    "pyannote_modules = [key for key in sys.modules.keys() if 'pyannote' in key]\n",
    "for mod in pyannote_modules:\n",
    "    del sys.modules[mod]\n",
    "print(f\"üîÑ Cleared {len(pyannote_modules)} cached pyannote modules\")\n",
    "\n",
    "# Step 2: Patch torch.load at the lowest level\n",
    "_original_torch_load = torch.serialization.load.__wrapped__ if hasattr(torch.serialization.load, '__wrapped__') else torch.serialization.load\n",
    "\n",
    "def _patched_load(f, map_location=None, pickle_module=None, *, weights_only=False, **kwargs):\n",
    "    # Force weights_only=False for pyannote compatibility\n",
    "    return _original_torch_load(f, map_location=map_location, pickle_module=pickle_module, weights_only=False, **kwargs)\n",
    "\n",
    "torch.serialization.load = _patched_load\n",
    "torch.load = _patched_load\n",
    "print(\"‚úÖ Patched torch.load with weights_only=False\")\n",
    "\n",
    "# Step 3: Now import pyannote fresh (will use patched torch.load)\n",
    "from pyannote.audio import Pipeline\n",
    "print(\"‚úÖ Imported pyannote.audio\")\n",
    "\n",
    "HF_TOKEN = os.environ.get(\"HUGGINGFACE_TOKEN\", \"\")\n",
    "\n",
    "if HF_TOKEN:\n",
    "    print(\"Loading pyannote speaker diarization model...\")\n",
    "    try:\n",
    "        # Note: newer pyannote uses 'token' instead of deprecated 'use_auth_token'\n",
    "        diarization_pipeline = Pipeline.from_pretrained(\n",
    "            \"pyannote/speaker-diarization-3.1\",\n",
    "            token=HF_TOKEN\n",
    "        )\n",
    "        # Move to GPU if available\n",
    "        if torch.cuda.is_available():\n",
    "            diarization_pipeline = diarization_pipeline.to(torch.device(\"cuda\"))\n",
    "            print(\"‚úÖ Diarization model loaded on GPU\")\n",
    "        else:\n",
    "            print(\"‚úÖ Diarization model loaded on CPU\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to load diarization model: {e}\")\n",
    "        print(\"   Make sure you accepted the license at:\")\n",
    "        print(\"   https://huggingface.co/pyannote/speaker-diarization-3.1\")\n",
    "        diarization_pipeline = None\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  HUGGINGFACE_TOKEN not set - diarization will be disabled\")\n",
    "    print(\"   Set it in Cell 4 to enable speaker diarization\")\n",
    "    diarization_pipeline = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transcription function\n",
    "import numpy as np\n",
    "\n",
    "def transcribe(audio_data, sample_rate=16000):\n",
    "    \"\"\"Transcribe audio using MERaLiON.\"\"\"\n",
    "    # Ensure float32 numpy array\n",
    "    if not isinstance(audio_data, np.ndarray):\n",
    "        audio_data = np.array(audio_data)\n",
    "    audio_data = audio_data.astype(np.float32)\n",
    "    \n",
    "    # Chat-style prompt for MERaLiON\n",
    "    prompt_template = \"Instruction: {query} \\nFollow the text instruction based on the following audio: <SpeechHere>\"\n",
    "    transcribe_prompt = \"\"\"Transcribe this Singlish speech using romanized text only. \n",
    "Do NOT use Chinese characters. \n",
    "Write Singlish words in romanized form: walao, shiok, lah, leh, lor, sia, paiseh, sian, etc.\n",
    "Output format: Speaker labels with romanized transcription.\"\"\"\n",
    "    \n",
    "    conversation = [[{\"role\": \"user\", \"content\": prompt_template.format(query=transcribe_prompt)}]]\n",
    "    chat_prompt = processor.tokenizer.apply_chat_template(\n",
    "        conversation=conversation,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    # Process inputs\n",
    "    inputs = processor(text=chat_prompt, audios=[audio_data])\n",
    "    \n",
    "    # Move to device\n",
    "    device = next(model.parameters()).device\n",
    "    dtype = next(model.parameters()).dtype\n",
    "    \n",
    "    def move_to_device(v):\n",
    "        if not hasattr(v, 'to'):\n",
    "            return v\n",
    "        v = v.to(device)\n",
    "        if v.is_floating_point():\n",
    "            v = v.to(dtype)\n",
    "        return v\n",
    "    \n",
    "    inputs = {k: move_to_device(v) for k, v in inputs.items()}\n",
    "    \n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(**inputs, max_new_tokens=256)\n",
    "    \n",
    "    # Decode\n",
    "    transcription = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    \n",
    "    # Clean up: extract only the model's response (matches backend _clean_model_output)\n",
    "    if \"model\\n\" in transcription:\n",
    "        transcription = transcription.split(\"model\\n\", 1)[-1]\n",
    "    \n",
    "    # Remove speaker markers like <Speaker1>:, <Speaker2>:, etc.\n",
    "    import re\n",
    "    transcription = re.sub(r'<Speaker\\d+>:\\s*', '', transcription)\n",
    "    # Remove <SpeechHere> tags\n",
    "    transcription = re.sub(r'<SpeechHere>', '', transcription)\n",
    "    # Clean bracketed words: !(walao)! -> walao, (lah) -> lah\n",
    "    transcription = re.sub(r'!\\(([^)]+)\\)!', r'\\1', transcription)\n",
    "    transcription = re.sub(r'\\(([a-zA-Z]+)\\)', r'\\1', transcription)\n",
    "    # Remove filler markers\n",
    "    transcription = re.sub(r'\\(err\\)', '', transcription, flags=re.IGNORECASE)\n",
    "    transcription = re.sub(r'\\(uh\\)', '', transcription, flags=re.IGNORECASE)\n",
    "    transcription = re.sub(r'\\(um\\)', '', transcription, flags=re.IGNORECASE)\n",
    "    # Clean up extra whitespace\n",
    "    transcription = re.sub(r'\\s+', ' ', transcription).strip()\n",
    "    \n",
    "    return transcription\n",
    "\n",
    "print(\"Transcription function ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post-processing functions\n",
    "import re\n",
    "from typing import Dict\n",
    "\n",
    "CORRECTIONS = {\n",
    "    # Walao variations\n",
    "    'while up': 'walao', 'wah lao eh': 'walao', 'wa lao eh': 'walao',\n",
    "    'wah lao': 'walao', 'wa lao': 'walao', 'wah low': 'walao',\n",
    "    'wa low': 'walao', 'while ah': 'walao', 'wah lau': 'walao',\n",
    "    'wa lau': 'walao', 'wah liao': 'walao', 'wa liao': 'walao',\n",
    "    'while low': 'walao', 'wah lei': 'walao', 'why lao': 'walao',\n",
    "    'why low': 'walao', 'wah la': 'walao',\n",
    "    # Vulgar - cheebai\n",
    "    'cheap buy': 'cheebai', 'chee bye': 'cheebai', 'chi bye': 'cheebai',\n",
    "    'chee bai': 'cheebai', 'chi bai': 'cheebai', 'chee by': 'cheebai',\n",
    "    'chi by': 'cheebai', 'cb': 'cheebai', 'c b': 'cheebai', 'see bee': 'cheebai',\n",
    "    # Vulgar - lanjiao\n",
    "    'lunch hour': 'lanjiao', 'lan jiao': 'lanjiao', 'lan chow': 'lanjiao',\n",
    "    'lan chiao': 'lanjiao', 'lun jiao': 'lanjiao', 'lan chio': 'lanjiao',\n",
    "    'lunchow': 'lanjiao', 'lan jio': 'lanjiao',\n",
    "    # Vulgar - kanina\n",
    "    'can nina': 'kanina', 'kar ni na': 'kanina', 'ka ni na': 'kanina',\n",
    "    'car nina': 'kanina', 'knn': 'kanina', 'k n n': 'kanina',\n",
    "    # Vulgar - nabei\n",
    "    'nah bay': 'nabei', 'na bei': 'nabei', 'nah bei': 'nabei',\n",
    "    'na beh': 'nabei', 'nah beh': 'nabei',\n",
    "    # Paiseh\n",
    "    'pie say': 'paiseh', 'pai seh': 'paiseh', 'pie seh': 'paiseh',\n",
    "    'pai say': 'paiseh', 'pie se': 'paiseh', 'pai se': 'paiseh', 'paise': 'paiseh',\n",
    "    # Shiok\n",
    "    'shook': 'shiok', 'she ok': 'shiok', 'shoe ok': 'shiok', 'shi ok': 'shiok',\n",
    "    # Alamak\n",
    "    'ala mak': 'alamak', 'allah mak': 'alamak', 'a la mak': 'alamak',\n",
    "    'allamak': 'alamak', 'aller mak': 'alamak',\n",
    "    # Aiyo/Aiyah\n",
    "    'ai yo': 'aiyo', 'ai yoh': 'aiyo', 'aiya': 'aiyah', 'ai ya': 'aiyah',\n",
    "    'eye yo': 'aiyo', 'aye yo': 'aiyo', 'ai yah': 'aiyah', 'eye yah': 'aiyah',\n",
    "    # Jialat\n",
    "    'jia lat': 'jialat', 'gia lat': 'jialat', 'jia lut': 'jialat',\n",
    "    'jee ah lat': 'jialat', 'gia lut': 'jialat',\n",
    "    # Bojio\n",
    "    'bo jio': 'bojio', 'boh jio': 'bojio', 'bo gio': 'bojio',\n",
    "    'never jio': 'bojio', 'boh gio': 'bojio',\n",
    "    # Sia/Sian\n",
    "    'see ya': 'sia', 'see ah': 'sia', 'siah': 'sia', 'si ah': 'sia',\n",
    "    'see an': 'sian', 'si an': 'sian', 'see en': 'sian', 'si en': 'sian',\n",
    "    # Other\n",
    "    'kia su': 'kiasu', 'key ah su': 'kiasu', 'kia si': 'kiasi', 'key ah si': 'kiasi',\n",
    "    'boh doh': 'bodoh', 'bo doh': 'bodoh', 'sua ku': 'suaku', 'swah ku': 'suaku',\n",
    "    'le pak': 'lepak', 'lay pak': 'lepak', 'chop': 'chope', 'ma kan': 'makan',\n",
    "    'go stan': 'gostan', 'go stun': 'gostan', 'si bei': 'sibei', 'see bay': 'sibei',\n",
    "    'si bay': 'sibei', 'ah tas': 'atas', 'ar tas': 'atas', 'kay poh': 'kaypoh',\n",
    "    'kae poh': 'kaypoh', 'kaypo': 'kaypoh', 'kpo': 'kaypoh',\n",
    "    'steady pom pi pi': 'steady', 'goon du': 'goondu', 'gun du': 'goondu',\n",
    "}\n",
    "\n",
    "WORD_CORRECTIONS = {\n",
    "    'la': 'lah', 'laa': 'lah', 'laaa': 'lah',\n",
    "    'low': 'lor', 'loh': 'lor',\n",
    "    # 'leh' is a distinct particle - don't convert to 'lah'\n",
    "    'ler': 'lah',\n",
    "    'seh': 'sia',\n",
    "    'arh': 'ah',\n",
    "    'err': 'eh',\n",
    "    'shio': 'shiok',\n",
    "}\n",
    "\n",
    "TARGET_WORDS = [\n",
    "    # Vulgar\n",
    "    'walao', 'cheebai', 'lanjiao', 'kanina', 'nabei',\n",
    "    # Particles\n",
    "    'lah', 'lor', 'sia', 'meh', 'leh', 'hor', 'ah', 'one', 'what', 'lei', 'ma',\n",
    "    # Exclamations\n",
    "    'wah', 'eh', 'huh', 'aiyo', 'aiyah', 'alamak',\n",
    "    # Colloquial\n",
    "    'can', 'cannot', 'paiseh', 'shiok', 'sian', 'bodoh', 'kiasu', 'kiasi',\n",
    "    'bojio', 'suaku', 'lepak', 'blur', 'goondu', 'cheem', 'chim',\n",
    "    # Actions\n",
    "    'chope', 'kena', 'makan', 'tahan', 'gostan', 'cabut', 'sabo', 'arrow',\n",
    "    # Intensifiers\n",
    "    'sibei', 'buay', 'jialat',\n",
    "    # Food/Drink\n",
    "    'kopi', 'teh', 'peng',\n",
    "    # Misc\n",
    "    'atas', 'kaypoh', 'steady', 'power', 'liao',\n",
    "]\n",
    "\n",
    "def apply_corrections(text: str) -> str:\n",
    "    if not text:\n",
    "        return text\n",
    "    result = text\n",
    "    for wrong, correct in sorted(CORRECTIONS.items(), key=lambda x: len(x[0]), reverse=True):\n",
    "        pattern = re.compile(re.escape(wrong), re.IGNORECASE)\n",
    "        result = pattern.sub(correct, result)\n",
    "    for wrong, correct in WORD_CORRECTIONS.items():\n",
    "        pattern = re.compile(r'\\b' + re.escape(wrong) + r'\\b', re.IGNORECASE)\n",
    "        result = pattern.sub(correct, result)\n",
    "    return result\n",
    "\n",
    "def count_target_words(text: str) -> Dict[str, int]:\n",
    "    if not text:\n",
    "        return {}\n",
    "    normalized = text.lower()\n",
    "    counts = {}\n",
    "    for word in TARGET_WORDS:\n",
    "        pattern = re.compile(r'(?<![a-zA-Z])' + re.escape(word) + r'(?![a-zA-Z])', re.IGNORECASE)\n",
    "        matches = pattern.findall(normalized)\n",
    "        if matches:\n",
    "            counts[word] = len(matches)\n",
    "    return counts\n",
    "\n",
    "print(\"Post-processing functions ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üî• MANUALLY KILL NGROK SESSIONS (Run this if ngrok errors persist)\n",
    "# This cell helps you kill ngrok sessions that pyngrok.kill() can't reach\n",
    "\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"KILLING ALL NGROK SESSIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Method 1: Kill via pyngrok\n",
    "try:\n",
    "    from pyngrok import ngrok\n",
    "    ngrok.kill()\n",
    "    print(\"‚úÖ Killed via pyngrok\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå pyngrok.kill() failed: {e}\")\n",
    "\n",
    "# Method 2: Kill processes (Linux/Mac/Colab)\n",
    "try:\n",
    "    result = subprocess.run(['pkill', '-9', '-f', 'ngrok'], \n",
    "                           capture_output=True, text=True, timeout=5)\n",
    "    if result.returncode == 0:\n",
    "        print(\"‚úÖ Killed ngrok processes via pkill\")\n",
    "    else:\n",
    "        print(\"‚ÑπÔ∏è  No ngrok processes found (or pkill not available)\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ÑπÔ∏è  pkill failed (normal on Windows/Colab): {e}\")\n",
    "\n",
    "# Method 3: Try psutil if available\n",
    "try:\n",
    "    import psutil\n",
    "    killed = 0\n",
    "    for proc in psutil.process_iter(['pid', 'name', 'cmdline']):\n",
    "        try:\n",
    "            cmdline = ' '.join(proc.info['cmdline'] or [])\n",
    "            if 'ngrok' in cmdline.lower():\n",
    "                proc.kill()\n",
    "                killed += 1\n",
    "                print(f\"‚úÖ Killed process PID {proc.info['pid']}\")\n",
    "        except (psutil.NoSuchProcess, psutil.AccessDenied, psutil.ZombieProcess):\n",
    "            pass\n",
    "    if killed == 0:\n",
    "        print(\"‚ÑπÔ∏è  No ngrok processes found via psutil\")\n",
    "except ImportError:\n",
    "    print(\"‚ÑπÔ∏è  psutil not installed (optional)\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ÑπÔ∏è  psutil failed: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚ö†Ô∏è  IF ERRORS STILL PERSIST:\")\n",
    "print(\"=\" * 60)\n",
    "print(\"1. Visit: https://dashboard.ngrok.com/agents\")\n",
    "print(\"2. Click 'Stop' on ALL active sessions\")\n",
    "print(\"3. Wait 10 seconds\")\n",
    "print(\"4. Then re-run the ngrok cell below\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Flask API server with transcription + diarization\n",
    "from flask import Flask, request, jsonify\n",
    "import librosa\n",
    "import io\n",
    "import base64\n",
    "import threading\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/health', methods=['GET'])\n",
    "def health():\n",
    "    diarization_status = \"available\" if diarization_pipeline else \"disabled\"\n",
    "    return jsonify({\n",
    "        \"status\": \"ok\", \n",
    "        \"model\": \"MERaLiON-2-3B\",\n",
    "        \"diarization\": diarization_status\n",
    "    })\n",
    "\n",
    "@app.route('/transcribe', methods=['POST'])\n",
    "def transcribe_endpoint():\n",
    "    try:\n",
    "        # Accept audio as base64 or file upload\n",
    "        if request.is_json:\n",
    "            data = request.get_json()\n",
    "            audio_b64 = data.get('audio')\n",
    "            audio_bytes = base64.b64decode(audio_b64)\n",
    "        else:\n",
    "            audio_file = request.files.get('audio')\n",
    "            audio_bytes = audio_file.read()\n",
    "        \n",
    "        # Load audio\n",
    "        audio_data, sr = librosa.load(io.BytesIO(audio_bytes), sr=16000)\n",
    "        \n",
    "        # Transcribe\n",
    "        raw_text = transcribe(audio_data)\n",
    "        \n",
    "        # Post-process\n",
    "        corrected = apply_corrections(raw_text)\n",
    "        counts = count_target_words(corrected)\n",
    "        \n",
    "        return jsonify({\n",
    "            \"raw_transcription\": raw_text,\n",
    "            \"corrected\": corrected,\n",
    "            \"word_counts\": counts,\n",
    "            \"total_singlish_words\": sum(counts.values())\n",
    "        })\n",
    "    except Exception as e:\n",
    "        return jsonify({\"error\": str(e)}), 500\n",
    "\n",
    "@app.route('/diarize', methods=['POST'])\n",
    "def diarize_endpoint():\n",
    "    \"\"\"Speaker diarization - segments audio by who spoke when.\"\"\"\n",
    "    if not diarization_pipeline:\n",
    "        return jsonify({\n",
    "            \"error\": \"Diarization not available. Set HUGGINGFACE_TOKEN in cell-3.\"\n",
    "        }), 503\n",
    "    \n",
    "    temp_path = None\n",
    "    try:\n",
    "        # Accept audio as base64 or file upload\n",
    "        if request.is_json:\n",
    "            data = request.get_json()\n",
    "            audio_b64 = data.get('audio')\n",
    "            audio_bytes = base64.b64decode(audio_b64)\n",
    "        else:\n",
    "            audio_file = request.files.get('audio')\n",
    "            audio_bytes = audio_file.read()\n",
    "        \n",
    "        # Save to temp file (pyannote needs file path)\n",
    "        with tempfile.NamedTemporaryFile(suffix='.wav', delete=False) as f:\n",
    "            temp_path = f.name\n",
    "            f.write(audio_bytes)\n",
    "        \n",
    "        # Run diarization\n",
    "        diarization_result = diarization_pipeline(temp_path)\n",
    "        \n",
    "        # Convert to list of segments\n",
    "        segments = []\n",
    "        for turn, _, speaker in diarization_result.itertracks(yield_label=True):\n",
    "            segments.append({\n",
    "                \"speaker_id\": speaker,\n",
    "                \"start_time\": round(turn.start, 3),\n",
    "                \"end_time\": round(turn.end, 3),\n",
    "                \"duration\": round(turn.end - turn.start, 3)\n",
    "            })\n",
    "        \n",
    "        # Get unique speakers\n",
    "        speakers = list(set(s[\"speaker_id\"] for s in segments))\n",
    "        \n",
    "        return jsonify({\n",
    "            \"segments\": segments,\n",
    "            \"speakers\": speakers,\n",
    "            \"num_speakers\": len(speakers),\n",
    "            \"num_segments\": len(segments)\n",
    "        })\n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return jsonify({\"error\": str(e)}), 500\n",
    "    finally:\n",
    "        # Clean up temp file\n",
    "        if temp_path and os.path.exists(temp_path):\n",
    "            try:\n",
    "                os.remove(temp_path)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "# Run Flask in background thread\n",
    "# Try different ports if 5000 is in use\n",
    "import socket\n",
    "\n",
    "def find_free_port(start_port=5000):\n",
    "    for port in range(start_port, start_port + 10):\n",
    "        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
    "            if s.connect_ex(('localhost', port)) != 0:\n",
    "                return port\n",
    "    return None\n",
    "\n",
    "PORT = find_free_port(5000)\n",
    "if PORT is None:\n",
    "    raise RuntimeError(\"Could not find a free port\")\n",
    "\n",
    "print(f\"Starting Flask server on port {PORT}...\")\n",
    "threading.Thread(target=lambda: app.run(host='0.0.0.0', port=PORT, use_reloader=False)).start()\n",
    "print(f\"Flask server started on port {PORT}!\")\n",
    "print(f\"\\nEndpoints available:\")\n",
    "print(f\"  GET  /health    - Check server status\")\n",
    "print(f\"  POST /transcribe - Transcribe audio\")\n",
    "print(f\"  POST /diarize   - Speaker diarization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expose server via ngrok\n",
    "import os\n",
    "from pyngrok import ngrok\n",
    "\n",
    "# Kill any existing ngrok tunnels first (free tier allows only 1 session)\n",
    "import subprocess\n",
    "import time\n",
    "\n",
    "# Method 1: Use pyngrok kill\n",
    "try:\n",
    "    ngrok.kill()\n",
    "    print(\"Killed existing ngrok sessions (pyngrok)\")\n",
    "    time.sleep(1)  # Wait a bit\n",
    "except Exception as e:\n",
    "    print(f\"pyngrok.kill() failed: {e}\")\n",
    "\n",
    "# Method 2: Kill ngrok processes directly\n",
    "try:\n",
    "    # Find and kill all ngrok processes\n",
    "    result = subprocess.run(['pkill', '-f', 'ngrok'], capture_output=True, text=True)\n",
    "    if result.returncode == 0:\n",
    "        print(\"Killed ngrok processes via pkill\")\n",
    "    time.sleep(1)\n",
    "except Exception as e:\n",
    "    print(f\"pkill failed (might be Windows/Colab): {e}\")\n",
    "    # Try Windows/alternative method\n",
    "    try:\n",
    "        import psutil\n",
    "        for proc in psutil.process_iter(['pid', 'name', 'cmdline']):\n",
    "            try:\n",
    "                if 'ngrok' in ' '.join(proc.info['cmdline'] or []).lower():\n",
    "                    proc.kill()\n",
    "                    print(f\"Killed ngrok process PID {proc.info['pid']}\")\n",
    "            except (psutil.NoSuchProcess, psutil.AccessDenied):\n",
    "                pass\n",
    "        time.sleep(1)\n",
    "    except ImportError:\n",
    "        print(\"psutil not available, skipping process kill\")\n",
    "    except Exception as e:\n",
    "        print(f\"Process kill failed: {e}\")\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è  If errors persist, manually kill ngrok:\")\n",
    "print(\"  1. Visit: https://dashboard.ngrok.com/agents\")\n",
    "print(\"  2. Stop all active sessions\")\n",
    "print(\"  3. Or run: ngrok kill (if ngrok CLI installed)\")\n",
    "print(\"  4. Or run: pkill -f ngrok (Linux/Mac)\")\n",
    "print(\"  5. Or run: taskkill /F /IM ngrok.exe (Windows)\")\n",
    "\n",
    "# Set authtoken from environment or paste directly\n",
    "NGROK_TOKEN = os.environ.get(\"NGROK_AUTHTOKEN\", \"YOUR_TOKEN_HERE\")\n",
    "ngrok.set_auth_token(NGROK_TOKEN)\n",
    "\n",
    "# Start tunnel (use PORT variable from previous cell, or default to 5000)\n",
    "# Check if PORT is defined, otherwise use default or try to detect Flask port\n",
    "import socket\n",
    "\n",
    "if 'PORT' not in globals():\n",
    "    # Try to find which port Flask is running on\n",
    "    flask_port = None\n",
    "    for port in range(5000, 5010):\n",
    "        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
    "            if s.connect_ex(('localhost', port)) == 0:\n",
    "                flask_port = port\n",
    "                break\n",
    "    \n",
    "    if flask_port:\n",
    "        PORT = flask_port\n",
    "        print(f\"Detected Flask running on port {PORT}\")\n",
    "    else:\n",
    "        PORT = 5000\n",
    "        print(f\"PORT not defined, using default port {PORT}\")\n",
    "        print(\"‚ö†Ô∏è  Make sure Flask server cell was run first!\")\n",
    "else:\n",
    "    print(f\"Using PORT={PORT} from Flask server cell\")\n",
    "\n",
    "try:\n",
    "    public_url = ngrok.connect(PORT).public_url\n",
    "except Exception as e:\n",
    "    error_msg = str(e)\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"‚ùå NGROK CONNECTION FAILED\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Error: {error_msg}\")\n",
    "    print(f\"Tried to connect to port {PORT}\")\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"üîß TO FIX THIS:\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(\"1. Run the cell ABOVE this one (the kill helper cell)\")\n",
    "    print(\"2. OR manually visit: https://dashboard.ngrok.com/agents\")\n",
    "    print(\"3. Stop ALL active ngrok sessions\")\n",
    "    print(\"4. Wait 10 seconds\")\n",
    "    print(\"5. Re-run THIS cell\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(\"\\nüí° The free ngrok tier only allows 1 session at a time.\")\n",
    "    print(\"   You must kill the existing session before starting a new one.\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # Don't raise - let user fix it and retry\n",
    "    public_url = None\n",
    "\n",
    "if public_url:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"‚úÖ ML API READY!\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"\\nPublic URL: {public_url}\")\n",
    "    print(f\"\\nSet in your backend .env:\")\n",
    "    print(f\"TRANSCRIPTION_API_URL={public_url}\")\n",
    "    print(f\"DIARIZATION_API_URL={public_url}\")\n",
    "    print(f\"\\nEndpoints:\")\n",
    "    print(f\"  GET  {public_url}/health     - Check server status\")\n",
    "    print(f\"  POST {public_url}/transcribe - Transcribe audio\")\n",
    "    print(f\"  POST {public_url}/diarize    - Speaker diarization\")\n",
    "else:\n",
    "    print(\"\\n‚ùå ngrok tunnel not created. Fix the error above and re-run this cell.\")\n",
    "print(f\"\\nKeep this notebook running!\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  !nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
