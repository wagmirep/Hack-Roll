{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel 'Python 3 (ipykernel)'. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details. Unable to get resolved server information for google.colab:colab:8a0e8c43-1184-4bc6-a69a-cca9a202321a"
     ]
    }
   ],
   "source": [
    "# âš ï¸ RUN THIS CELL TO RESET RUNTIME (only if you have dependency issues)\n",
    "# This disconnects and deletes the runtime - you'll need to reconnect after\n",
    "from google.colab import runtime\n",
    "runtime.unassign()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LahStats - ML Server (Transcription + Diarization)\n",
    "\n",
    "Runs MERaLiON (transcription) and pyannote (speaker diarization) as a persistent API server.\n",
    "\n",
    "**Setup:**\n",
    "1. Run all cells in order\n",
    "2. Copy the ngrok URL to your backend .env\n",
    "3. Keep this notebook running during the demo\n",
    "\n",
    "**Endpoints:**\n",
    "- `GET /health` - Check server status\n",
    "- `POST /transcribe` - Transcribe audio\n",
    "- `POST /diarize` - Speaker diarization (who spoke when)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.9/44.9 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m91.2/91.2 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m59.6/59.6 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m892.9/892.9 kB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m887.9/887.9 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m59.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m88.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m594.3/594.3 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m129.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m88.0/88.0 MB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m954.8/954.8 kB\u001b[0m \u001b[31m77.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m193.1/193.1 MB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m80.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m63.6/63.6 MB\u001b[0m \u001b[31m42.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m267.5/267.5 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m288.2/288.2 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m322.4/322.4 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m39.3/39.3 MB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m155.6/155.6 MB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m846.0/846.0 kB\u001b[0m \u001b[31m43.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m132.6/132.6 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m220.0/220.0 kB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m53.7/53.7 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m49.7/49.7 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.8/127.8 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m983.2/983.2 kB\u001b[0m \u001b[31m73.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m132.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m404.7/404.7 kB\u001b[0m \u001b[31m42.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m136.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m849.5/849.5 kB\u001b[0m \u001b[31m63.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Building wheel for julius (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "peft 0.18.0 requires transformers, which is not installed.\n",
      "sentence-transformers 5.2.0 requires transformers<6.0.0,>=4.41.0, which is not installed.\n",
      "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.3.3 which is incompatible.\n",
      "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.4.1 which is incompatible.\n",
      "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.4.1 which is incompatible.\n",
      "torchvision 0.24.0+cu126 requires torch==2.9.0, but you have torch 2.8.0 which is incompatible.\n",
      "tensorflow 2.19.0 requires numpy<2.2.0,>=1.26.0, but you have numpy 2.4.1 which is incompatible.\n",
      "opentelemetry-exporter-gcp-logging 1.11.0a0 requires opentelemetry-sdk<1.39.0,>=1.35.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\n",
      "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.4.1 which is incompatible.\n",
      "google-adk 1.21.0 requires opentelemetry-api<=1.37.0,>=1.37.0, but you have opentelemetry-api 1.39.1 which is incompatible.\n",
      "google-adk 1.21.0 requires opentelemetry-sdk<=1.37.0,>=1.37.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\n",
      "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.4.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m60.9/60.9 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m92.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m19.2/19.2 MB\u001b[0m \u001b[31m112.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m126.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pyannote-metrics 4.0.0 requires numpy>=2.2.2, but you have numpy 2.0.2 which is incompatible.\n",
      "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.3.3 which is incompatible.\n",
      "torchvision 0.24.0+cu126 requires torch==2.9.0, but you have torch 2.8.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "============================================================\n",
      "âœ… Dependencies installed!\n",
      "âš ï¸  NOW RESTART RUNTIME:\n",
      "   Runtime â†’ Restart runtime\n",
      "   Then run Cell 3 onwards (skip this cell)\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Install dependencies FIRST (before importing torch)\n",
    "# Install matching torch + torchvision + torchaudio together\n",
    "!pip uninstall -y torch torchvision torchaudio transformers -q 2>/dev/null || true\n",
    "!pip install -q torch==2.8.0 torchvision==0.23.0 torchaudio==2.8.0 --index-url https://download.pytorch.org/whl/cu121\n",
    "!pip install -q pyannote.audio --no-deps\n",
    "!pip install -q pyannote.core pyannote.database pyannote.pipeline asteroid-filterbanks einops hmmlearn hyperpyyaml lightning lightning-fabric omegaconf optuna pytorch_metric_learning rich schedulefree semver soundfile speechbrain tensorboardX\n",
    "!pip install -q \"transformers>=4.40.0,<4.50.0\" accelerate librosa flask pyngrok\n",
    "\n",
    "# Restart message\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"âœ… Dependencies installed!\")\n",
    "print(\"âš ï¸  NOW RESTART RUNTIME:\")\n",
    "print(\"   Runtime â†’ Restart runtime\")\n",
    "print(\"   Then run Cell 3 onwards (skip this cell)\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "GPU: NVIDIA A100-SXM4-40GB\n",
      "Memory: 42.5 GB\n",
      "Torch version: 2.8.0+cu128\n"
     ]
    }
   ],
   "source": [
    "# Check GPU availability (run after restart)\n",
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    print(f\"Torch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Set tokens - paste your tokens here (this notebook is gitignored)\nimport os\n\n# Ngrok token for public URL\nos.environ[\"NGROK_AUTHTOKEN\"] = \"YOUR_NGROK_TOKEN_HERE\"  # <-- Paste your ngrok token here\n\n# HuggingFace token for pyannote (get from https://huggingface.co/settings/tokens)\n# Also accept pyannote license at https://huggingface.co/pyannote/speaker-diarization-3.1\nos.environ[\"HUGGINGFACE_TOKEN\"] = \"YOUR_HUGGINGFACE_TOKEN_HERE\"  # <-- Paste your HF token here\n\nprint(\"NGROK_AUTHTOKEN:\", \"âœ… Set\" if os.environ.get(\"NGROK_AUTHTOKEN\") else \"âŒ Missing\")\nprint(\"HUGGINGFACE_TOKEN:\", \"âœ… Set\" if os.environ.get(\"HUGGINGFACE_TOKEN\") else \"âŒ Missing (needed for diarization)\")"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Failed to import transformers.models.auto.processing_auto because of the following error (look up to see its traceback):\noperator torchvision::nms does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1862\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1863\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1864\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.12/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/auto/processing_auto.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0mfeature_extraction_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFeatureExtractionMixin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0mimage_processing_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImageProcessingMixin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0mprocessing_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mProcessorMixin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/image_processing_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mimage_processing_base\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBatchFeature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mImageProcessingMixin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mimage_transforms\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcenter_crop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrescale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mimage_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mChannelDimension\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_image_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/image_transforms.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m from .image_utils import (\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mChannelDimension\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/image_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_torchvision_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mio\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtorchvision_io\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInterpolationMode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mextension\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_HAS_OPS\u001b[0m  \u001b[0;31m# usort:skip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_meta_registrations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutils\u001b[0m  \u001b[0;31m# usort:skip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/_meta_registrations.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m \u001b[0;34m@\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlibrary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister_fake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"torchvision::nms\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmeta_nms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miou_threshold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/library.py\u001b[0m in \u001b[0;36mregister\u001b[0;34m(func)\u001b[0m\n\u001b[1;32m   1068\u001b[0m             \u001b[0muse_lib\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1069\u001b[0;31m         use_lib._register_fake(\n\u001b[0m\u001b[1;32m   1070\u001b[0m             \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_override\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mallow_override\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/library.py\u001b[0m in \u001b[0;36m_register_fake\u001b[0;34m(self, op_name, fn, _stacklevel, allow_override)\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m         handle = entry.fake_impl.register(\n\u001b[0m\u001b[1;32m    220\u001b[0m             \u001b[0mfunc_to_register\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_override\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mallow_override\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_library/fake_impl.py\u001b[0m in \u001b[0;36mregister\u001b[0;34m(self, func, source, lib, allow_override)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 )\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch_has_kernel_for_dispatch_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqualname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Meta\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m                 raise RuntimeError(\n",
      "\u001b[0;31mRuntimeError\u001b[0m: operator torchvision::nms does not exist",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-502340563.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load MERaLiON-2-3B-ASR model (smaller, fits on most GPUs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Use 3B instead of 10B to avoid OOM errors - matches backend service\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoProcessor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoModelForSpeechSeq2Seq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1850\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1851\u001b[0m             \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1852\u001b[0;31m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1853\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1854\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1849\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPlaceholder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1851\u001b[0;31m             \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1852\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1853\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1863\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1864\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1865\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m   1866\u001b[0m                 \u001b[0;34mf\"Failed to import {self.__name__}.{module_name} because of the following error (look up to see its\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1867\u001b[0m                 \u001b[0;34mf\" traceback):\\n{e}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to import transformers.models.auto.processing_auto because of the following error (look up to see its traceback):\noperator torchvision::nms does not exist"
     ]
    }
   ],
   "source": [
    "# Load MERaLiON-2-3B-ASR model (smaller, fits on most GPUs)\n",
    "# Use 3B instead of 10B to avoid OOM errors - matches backend service\n",
    "from transformers import AutoProcessor, AutoModelForSpeechSeq2Seq\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "# Clear CUDA cache before loading\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "MODEL_NAME = \"MERaLiON/MERaLiON-2-3B\"  # Changed from 10B to 3B to avoid OOM\n",
    "\n",
    "print(\"Loading processor...\")\n",
    "processor = AutoProcessor.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "\n",
    "print(\"Loading model (this may take a few minutes)...\")\n",
    "print(f\"Using model: {MODEL_NAME}\")\n",
    "\n",
    "# Check GPU memory\n",
    "if torch.cuda.is_available():\n",
    "    total_mem = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"GPU Memory: {total_mem:.1f} GB\")\n",
    "    \n",
    "    # Load with float16 for GPU\n",
    "    model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        torch_dtype=torch.float16,\n",
    "        trust_remote_code=True,\n",
    "        attn_implementation=\"eager\",\n",
    "        low_cpu_mem_usage=True,\n",
    "        device_map=\"auto\",  # Automatic device placement\n",
    "    )\n",
    "    print(f\"Model loaded on GPU (float16)\")\n",
    "else:\n",
    "    # CPU fallback\n",
    "    print(\"No GPU available, loading for CPU...\")\n",
    "    model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        torch_dtype=torch.float32,\n",
    "        trust_remote_code=True,\n",
    "        attn_implementation=\"eager\",\n",
    "    )\n",
    "    print(\"Model loaded on CPU\")\n",
    "\n",
    "model.eval()  # Set to evaluation mode\n",
    "\n",
    "print(f\"Model loaded on {next(model.parameters()).device}!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pyannote speaker diarization model\n",
    "import os\n",
    "from pyannote.audio import Pipeline\n",
    "\n",
    "HF_TOKEN = os.environ.get(\"HUGGINGFACE_TOKEN\", \"\")\n",
    "\n",
    "if HF_TOKEN:\n",
    "    print(\"Loading pyannote speaker diarization model...\")\n",
    "    try:\n",
    "        diarization_pipeline = Pipeline.from_pretrained(\n",
    "            \"pyannote/speaker-diarization-3.1\",\n",
    "            use_auth_token=HF_TOKEN\n",
    "        )\n",
    "        # Move to GPU if available\n",
    "        if torch.cuda.is_available():\n",
    "            diarization_pipeline = diarization_pipeline.to(torch.device(\"cuda\"))\n",
    "            print(\"âœ… Diarization model loaded on GPU\")\n",
    "        else:\n",
    "            print(\"âœ… Diarization model loaded on CPU\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Failed to load diarization model: {e}\")\n",
    "        print(\"   Make sure you accepted the license at:\")\n",
    "        print(\"   https://huggingface.co/pyannote/speaker-diarization-3.1\")\n",
    "        diarization_pipeline = None\n",
    "else:\n",
    "    print(\"âš ï¸  HUGGINGFACE_TOKEN not set - diarization will be disabled\")\n",
    "    print(\"   Set it in cell-3 to enable speaker diarization\")\n",
    "    diarization_pipeline = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transcription function\n",
    "import numpy as np\n",
    "\n",
    "def transcribe(audio_data, sample_rate=16000):\n",
    "    \"\"\"Transcribe audio using MERaLiON.\"\"\"\n",
    "    # Ensure float32 numpy array\n",
    "    if not isinstance(audio_data, np.ndarray):\n",
    "        audio_data = np.array(audio_data)\n",
    "    audio_data = audio_data.astype(np.float32)\n",
    "    \n",
    "    # Chat-style prompt for MERaLiON\n",
    "    prompt_template = \"Instruction: {query} \\nFollow the text instruction based on the following audio: <SpeechHere>\"\n",
    "    transcribe_prompt = \"\"\"Transcribe this Singlish speech using romanized text only. \n",
    "Do NOT use Chinese characters. \n",
    "Write Singlish words in romanized form: walao, shiok, lah, leh, lor, sia, paiseh, sian, etc.\n",
    "Output format: Speaker labels with romanized transcription.\"\"\"\n",
    "    \n",
    "    conversation = [[{\"role\": \"user\", \"content\": prompt_template.format(query=transcribe_prompt)}]]\n",
    "    chat_prompt = processor.tokenizer.apply_chat_template(\n",
    "        conversation=conversation,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    # Process inputs\n",
    "    inputs = processor(text=chat_prompt, audios=[audio_data])\n",
    "    \n",
    "    # Move to device\n",
    "    device = next(model.parameters()).device\n",
    "    dtype = next(model.parameters()).dtype\n",
    "    \n",
    "    def move_to_device(v):\n",
    "        if not hasattr(v, 'to'):\n",
    "            return v\n",
    "        v = v.to(device)\n",
    "        if v.is_floating_point():\n",
    "            v = v.to(dtype)\n",
    "        return v\n",
    "    \n",
    "    inputs = {k: move_to_device(v) for k, v in inputs.items()}\n",
    "    \n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(**inputs, max_new_tokens=256)\n",
    "    \n",
    "    # Decode\n",
    "    transcription = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    \n",
    "    # Clean up: extract only the model's response (matches backend _clean_model_output)\n",
    "    if \"model\\n\" in transcription:\n",
    "        transcription = transcription.split(\"model\\n\", 1)[-1]\n",
    "    \n",
    "    # Remove speaker markers like <Speaker1>:, <Speaker2>:, etc.\n",
    "    import re\n",
    "    transcription = re.sub(r'<Speaker\\d+>:\\s*', '', transcription)\n",
    "    # Remove <SpeechHere> tags\n",
    "    transcription = re.sub(r'<SpeechHere>', '', transcription)\n",
    "    # Clean bracketed words: !(walao)! -> walao, (lah) -> lah\n",
    "    transcription = re.sub(r'!\\(([^)]+)\\)!', r'\\1', transcription)\n",
    "    transcription = re.sub(r'\\(([a-zA-Z]+)\\)', r'\\1', transcription)\n",
    "    # Remove filler markers\n",
    "    transcription = re.sub(r'\\(err\\)', '', transcription, flags=re.IGNORECASE)\n",
    "    transcription = re.sub(r'\\(uh\\)', '', transcription, flags=re.IGNORECASE)\n",
    "    transcription = re.sub(r'\\(um\\)', '', transcription, flags=re.IGNORECASE)\n",
    "    # Clean up extra whitespace\n",
    "    transcription = re.sub(r'\\s+', ' ', transcription).strip()\n",
    "    \n",
    "    return transcription\n",
    "\n",
    "print(\"Transcription function ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post-processing functions\n",
    "import re\n",
    "from typing import Dict\n",
    "\n",
    "CORRECTIONS = {\n",
    "    # Walao variations\n",
    "    'while up': 'walao', 'wah lao eh': 'walao', 'wa lao eh': 'walao',\n",
    "    'wah lao': 'walao', 'wa lao': 'walao', 'wah low': 'walao',\n",
    "    'wa low': 'walao', 'while ah': 'walao', 'wah lau': 'walao',\n",
    "    'wa lau': 'walao', 'wah liao': 'walao', 'wa liao': 'walao',\n",
    "    'while low': 'walao', 'wah lei': 'walao', 'why lao': 'walao',\n",
    "    'why low': 'walao', 'wah la': 'walao',\n",
    "    # Vulgar - cheebai\n",
    "    'cheap buy': 'cheebai', 'chee bye': 'cheebai', 'chi bye': 'cheebai',\n",
    "    'chee bai': 'cheebai', 'chi bai': 'cheebai', 'chee by': 'cheebai',\n",
    "    'chi by': 'cheebai', 'cb': 'cheebai', 'c b': 'cheebai', 'see bee': 'cheebai',\n",
    "    # Vulgar - lanjiao\n",
    "    'lunch hour': 'lanjiao', 'lan jiao': 'lanjiao', 'lan chow': 'lanjiao',\n",
    "    'lan chiao': 'lanjiao', 'lun jiao': 'lanjiao', 'lan chio': 'lanjiao',\n",
    "    'lunchow': 'lanjiao', 'lan jio': 'lanjiao',\n",
    "    # Vulgar - kanina\n",
    "    'can nina': 'kanina', 'kar ni na': 'kanina', 'ka ni na': 'kanina',\n",
    "    'car nina': 'kanina', 'knn': 'kanina', 'k n n': 'kanina',\n",
    "    # Vulgar - nabei\n",
    "    'nah bay': 'nabei', 'na bei': 'nabei', 'nah bei': 'nabei',\n",
    "    'na beh': 'nabei', 'nah beh': 'nabei',\n",
    "    # Paiseh\n",
    "    'pie say': 'paiseh', 'pai seh': 'paiseh', 'pie seh': 'paiseh',\n",
    "    'pai say': 'paiseh', 'pie se': 'paiseh', 'pai se': 'paiseh', 'paise': 'paiseh',\n",
    "    # Shiok\n",
    "    'shook': 'shiok', 'she ok': 'shiok', 'shoe ok': 'shiok', 'shi ok': 'shiok',\n",
    "    # Alamak\n",
    "    'ala mak': 'alamak', 'allah mak': 'alamak', 'a la mak': 'alamak',\n",
    "    'allamak': 'alamak', 'aller mak': 'alamak',\n",
    "    # Aiyo/Aiyah\n",
    "    'ai yo': 'aiyo', 'ai yoh': 'aiyo', 'aiya': 'aiyah', 'ai ya': 'aiyah',\n",
    "    'eye yo': 'aiyo', 'aye yo': 'aiyo', 'ai yah': 'aiyah', 'eye yah': 'aiyah',\n",
    "    # Jialat\n",
    "    'jia lat': 'jialat', 'gia lat': 'jialat', 'jia lut': 'jialat',\n",
    "    'jee ah lat': 'jialat', 'gia lut': 'jialat',\n",
    "    # Bojio\n",
    "    'bo jio': 'bojio', 'boh jio': 'bojio', 'bo gio': 'bojio',\n",
    "    'never jio': 'bojio', 'boh gio': 'bojio',\n",
    "    # Sia/Sian\n",
    "    'see ya': 'sia', 'see ah': 'sia', 'siah': 'sia', 'si ah': 'sia',\n",
    "    'see an': 'sian', 'si an': 'sian', 'see en': 'sian', 'si en': 'sian',\n",
    "    # Other\n",
    "    'kia su': 'kiasu', 'key ah su': 'kiasu', 'kia si': 'kiasi', 'key ah si': 'kiasi',\n",
    "    'boh doh': 'bodoh', 'bo doh': 'bodoh', 'sua ku': 'suaku', 'swah ku': 'suaku',\n",
    "    'le pak': 'lepak', 'lay pak': 'lepak', 'chop': 'chope', 'ma kan': 'makan',\n",
    "    'go stan': 'gostan', 'go stun': 'gostan', 'si bei': 'sibei', 'see bay': 'sibei',\n",
    "    'si bay': 'sibei', 'ah tas': 'atas', 'ar tas': 'atas', 'kay poh': 'kaypoh',\n",
    "    'kae poh': 'kaypoh', 'kaypo': 'kaypoh', 'kpo': 'kaypoh',\n",
    "    'steady pom pi pi': 'steady', 'goon du': 'goondu', 'gun du': 'goondu',\n",
    "}\n",
    "\n",
    "WORD_CORRECTIONS = {\n",
    "    'la': 'lah', 'laa': 'lah', 'laaa': 'lah',\n",
    "    'low': 'lor', 'loh': 'lor',\n",
    "    # 'leh' is a distinct particle - don't convert to 'lah'\n",
    "    'ler': 'lah',\n",
    "    'seh': 'sia',\n",
    "    'arh': 'ah',\n",
    "    'err': 'eh',\n",
    "    'shio': 'shiok',\n",
    "}\n",
    "\n",
    "TARGET_WORDS = [\n",
    "    # Vulgar\n",
    "    'walao', 'cheebai', 'lanjiao', 'kanina', 'nabei',\n",
    "    # Particles\n",
    "    'lah', 'lor', 'sia', 'meh', 'leh', 'hor', 'ah', 'one', 'what', 'lei', 'ma',\n",
    "    # Exclamations\n",
    "    'wah', 'eh', 'huh', 'aiyo', 'aiyah', 'alamak',\n",
    "    # Colloquial\n",
    "    'can', 'cannot', 'paiseh', 'shiok', 'sian', 'bodoh', 'kiasu', 'kiasi',\n",
    "    'bojio', 'suaku', 'lepak', 'blur', 'goondu', 'cheem', 'chim',\n",
    "    # Actions\n",
    "    'chope', 'kena', 'makan', 'tahan', 'gostan', 'cabut', 'sabo', 'arrow',\n",
    "    # Intensifiers\n",
    "    'sibei', 'buay', 'jialat',\n",
    "    # Food/Drink\n",
    "    'kopi', 'teh', 'peng',\n",
    "    # Misc\n",
    "    'atas', 'kaypoh', 'steady', 'power', 'liao',\n",
    "]\n",
    "\n",
    "def apply_corrections(text: str) -> str:\n",
    "    if not text:\n",
    "        return text\n",
    "    result = text\n",
    "    for wrong, correct in sorted(CORRECTIONS.items(), key=lambda x: len(x[0]), reverse=True):\n",
    "        pattern = re.compile(re.escape(wrong), re.IGNORECASE)\n",
    "        result = pattern.sub(correct, result)\n",
    "    for wrong, correct in WORD_CORRECTIONS.items():\n",
    "        pattern = re.compile(r'\\b' + re.escape(wrong) + r'\\b', re.IGNORECASE)\n",
    "        result = pattern.sub(correct, result)\n",
    "    return result\n",
    "\n",
    "def count_target_words(text: str) -> Dict[str, int]:\n",
    "    if not text:\n",
    "        return {}\n",
    "    normalized = text.lower()\n",
    "    counts = {}\n",
    "    for word in TARGET_WORDS:\n",
    "        pattern = re.compile(r'(?<![a-zA-Z])' + re.escape(word) + r'(?![a-zA-Z])', re.IGNORECASE)\n",
    "        matches = pattern.findall(normalized)\n",
    "        if matches:\n",
    "            counts[word] = len(matches)\n",
    "    return counts\n",
    "\n",
    "print(\"Post-processing functions ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ”¥ MANUALLY KILL NGROK SESSIONS (Run this if ngrok errors persist)\n",
    "# This cell helps you kill ngrok sessions that pyngrok.kill() can't reach\n",
    "\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"KILLING ALL NGROK SESSIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Method 1: Kill via pyngrok\n",
    "try:\n",
    "    from pyngrok import ngrok\n",
    "    ngrok.kill()\n",
    "    print(\"âœ… Killed via pyngrok\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ pyngrok.kill() failed: {e}\")\n",
    "\n",
    "# Method 2: Kill processes (Linux/Mac/Colab)\n",
    "try:\n",
    "    result = subprocess.run(['pkill', '-9', '-f', 'ngrok'], \n",
    "                           capture_output=True, text=True, timeout=5)\n",
    "    if result.returncode == 0:\n",
    "        print(\"âœ… Killed ngrok processes via pkill\")\n",
    "    else:\n",
    "        print(\"â„¹ï¸  No ngrok processes found (or pkill not available)\")\n",
    "except Exception as e:\n",
    "    print(f\"â„¹ï¸  pkill failed (normal on Windows/Colab): {e}\")\n",
    "\n",
    "# Method 3: Try psutil if available\n",
    "try:\n",
    "    import psutil\n",
    "    killed = 0\n",
    "    for proc in psutil.process_iter(['pid', 'name', 'cmdline']):\n",
    "        try:\n",
    "            cmdline = ' '.join(proc.info['cmdline'] or [])\n",
    "            if 'ngrok' in cmdline.lower():\n",
    "                proc.kill()\n",
    "                killed += 1\n",
    "                print(f\"âœ… Killed process PID {proc.info['pid']}\")\n",
    "        except (psutil.NoSuchProcess, psutil.AccessDenied, psutil.ZombieProcess):\n",
    "            pass\n",
    "    if killed == 0:\n",
    "        print(\"â„¹ï¸  No ngrok processes found via psutil\")\n",
    "except ImportError:\n",
    "    print(\"â„¹ï¸  psutil not installed (optional)\")\n",
    "except Exception as e:\n",
    "    print(f\"â„¹ï¸  psutil failed: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"âš ï¸  IF ERRORS STILL PERSIST:\")\n",
    "print(\"=\" * 60)\n",
    "print(\"1. Visit: https://dashboard.ngrok.com/agents\")\n",
    "print(\"2. Click 'Stop' on ALL active sessions\")\n",
    "print(\"3. Wait 10 seconds\")\n",
    "print(\"4. Then re-run the ngrok cell below\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Flask API server with transcription + diarization\n",
    "from flask import Flask, request, jsonify\n",
    "import librosa\n",
    "import io\n",
    "import base64\n",
    "import threading\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/health', methods=['GET'])\n",
    "def health():\n",
    "    diarization_status = \"available\" if diarization_pipeline else \"disabled\"\n",
    "    return jsonify({\n",
    "        \"status\": \"ok\", \n",
    "        \"model\": \"MERaLiON-2-3B\",\n",
    "        \"diarization\": diarization_status\n",
    "    })\n",
    "\n",
    "@app.route('/transcribe', methods=['POST'])\n",
    "def transcribe_endpoint():\n",
    "    try:\n",
    "        # Accept audio as base64 or file upload\n",
    "        if request.is_json:\n",
    "            data = request.get_json()\n",
    "            audio_b64 = data.get('audio')\n",
    "            audio_bytes = base64.b64decode(audio_b64)\n",
    "        else:\n",
    "            audio_file = request.files.get('audio')\n",
    "            audio_bytes = audio_file.read()\n",
    "        \n",
    "        # Load audio\n",
    "        audio_data, sr = librosa.load(io.BytesIO(audio_bytes), sr=16000)\n",
    "        \n",
    "        # Transcribe\n",
    "        raw_text = transcribe(audio_data)\n",
    "        \n",
    "        # Post-process\n",
    "        corrected = apply_corrections(raw_text)\n",
    "        counts = count_target_words(corrected)\n",
    "        \n",
    "        return jsonify({\n",
    "            \"raw_transcription\": raw_text,\n",
    "            \"corrected\": corrected,\n",
    "            \"word_counts\": counts,\n",
    "            \"total_singlish_words\": sum(counts.values())\n",
    "        })\n",
    "    except Exception as e:\n",
    "        return jsonify({\"error\": str(e)}), 500\n",
    "\n",
    "@app.route('/diarize', methods=['POST'])\n",
    "def diarize_endpoint():\n",
    "    \"\"\"Speaker diarization - segments audio by who spoke when.\"\"\"\n",
    "    if not diarization_pipeline:\n",
    "        return jsonify({\n",
    "            \"error\": \"Diarization not available. Set HUGGINGFACE_TOKEN in cell-3.\"\n",
    "        }), 503\n",
    "    \n",
    "    temp_path = None\n",
    "    try:\n",
    "        # Accept audio as base64 or file upload\n",
    "        if request.is_json:\n",
    "            data = request.get_json()\n",
    "            audio_b64 = data.get('audio')\n",
    "            audio_bytes = base64.b64decode(audio_b64)\n",
    "        else:\n",
    "            audio_file = request.files.get('audio')\n",
    "            audio_bytes = audio_file.read()\n",
    "        \n",
    "        # Save to temp file (pyannote needs file path)\n",
    "        with tempfile.NamedTemporaryFile(suffix='.wav', delete=False) as f:\n",
    "            temp_path = f.name\n",
    "            f.write(audio_bytes)\n",
    "        \n",
    "        # Run diarization\n",
    "        diarization_result = diarization_pipeline(temp_path)\n",
    "        \n",
    "        # Convert to list of segments\n",
    "        segments = []\n",
    "        for turn, _, speaker in diarization_result.itertracks(yield_label=True):\n",
    "            segments.append({\n",
    "                \"speaker_id\": speaker,\n",
    "                \"start_time\": round(turn.start, 3),\n",
    "                \"end_time\": round(turn.end, 3),\n",
    "                \"duration\": round(turn.end - turn.start, 3)\n",
    "            })\n",
    "        \n",
    "        # Get unique speakers\n",
    "        speakers = list(set(s[\"speaker_id\"] for s in segments))\n",
    "        \n",
    "        return jsonify({\n",
    "            \"segments\": segments,\n",
    "            \"speakers\": speakers,\n",
    "            \"num_speakers\": len(speakers),\n",
    "            \"num_segments\": len(segments)\n",
    "        })\n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return jsonify({\"error\": str(e)}), 500\n",
    "    finally:\n",
    "        # Clean up temp file\n",
    "        if temp_path and os.path.exists(temp_path):\n",
    "            try:\n",
    "                os.remove(temp_path)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "# Run Flask in background thread\n",
    "# Try different ports if 5000 is in use\n",
    "import socket\n",
    "\n",
    "def find_free_port(start_port=5000):\n",
    "    for port in range(start_port, start_port + 10):\n",
    "        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
    "            if s.connect_ex(('localhost', port)) != 0:\n",
    "                return port\n",
    "    return None\n",
    "\n",
    "PORT = find_free_port(5000)\n",
    "if PORT is None:\n",
    "    raise RuntimeError(\"Could not find a free port\")\n",
    "\n",
    "print(f\"Starting Flask server on port {PORT}...\")\n",
    "threading.Thread(target=lambda: app.run(host='0.0.0.0', port=PORT, use_reloader=False)).start()\n",
    "print(f\"Flask server started on port {PORT}!\")\n",
    "print(f\"\\nEndpoints available:\")\n",
    "print(f\"  GET  /health    - Check server status\")\n",
    "print(f\"  POST /transcribe - Transcribe audio\")\n",
    "print(f\"  POST /diarize   - Speaker diarization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expose server via ngrok\n",
    "import os\n",
    "from pyngrok import ngrok\n",
    "\n",
    "# Kill any existing ngrok tunnels first (free tier allows only 1 session)\n",
    "import subprocess\n",
    "import time\n",
    "\n",
    "# Method 1: Use pyngrok kill\n",
    "try:\n",
    "    ngrok.kill()\n",
    "    print(\"Killed existing ngrok sessions (pyngrok)\")\n",
    "    time.sleep(1)  # Wait a bit\n",
    "except Exception as e:\n",
    "    print(f\"pyngrok.kill() failed: {e}\")\n",
    "\n",
    "# Method 2: Kill ngrok processes directly\n",
    "try:\n",
    "    # Find and kill all ngrok processes\n",
    "    result = subprocess.run(['pkill', '-f', 'ngrok'], capture_output=True, text=True)\n",
    "    if result.returncode == 0:\n",
    "        print(\"Killed ngrok processes via pkill\")\n",
    "    time.sleep(1)\n",
    "except Exception as e:\n",
    "    print(f\"pkill failed (might be Windows/Colab): {e}\")\n",
    "    # Try Windows/alternative method\n",
    "    try:\n",
    "        import psutil\n",
    "        for proc in psutil.process_iter(['pid', 'name', 'cmdline']):\n",
    "            try:\n",
    "                if 'ngrok' in ' '.join(proc.info['cmdline'] or []).lower():\n",
    "                    proc.kill()\n",
    "                    print(f\"Killed ngrok process PID {proc.info['pid']}\")\n",
    "            except (psutil.NoSuchProcess, psutil.AccessDenied):\n",
    "                pass\n",
    "        time.sleep(1)\n",
    "    except ImportError:\n",
    "        print(\"psutil not available, skipping process kill\")\n",
    "    except Exception as e:\n",
    "        print(f\"Process kill failed: {e}\")\n",
    "\n",
    "print(\"\\nâš ï¸  If errors persist, manually kill ngrok:\")\n",
    "print(\"  1. Visit: https://dashboard.ngrok.com/agents\")\n",
    "print(\"  2. Stop all active sessions\")\n",
    "print(\"  3. Or run: ngrok kill (if ngrok CLI installed)\")\n",
    "print(\"  4. Or run: pkill -f ngrok (Linux/Mac)\")\n",
    "print(\"  5. Or run: taskkill /F /IM ngrok.exe (Windows)\")\n",
    "\n",
    "# Set authtoken from environment or paste directly\n",
    "NGROK_TOKEN = os.environ.get(\"NGROK_AUTHTOKEN\", \"YOUR_TOKEN_HERE\")\n",
    "ngrok.set_auth_token(NGROK_TOKEN)\n",
    "\n",
    "# Start tunnel (use PORT variable from previous cell, or default to 5000)\n",
    "# Check if PORT is defined, otherwise use default or try to detect Flask port\n",
    "import socket\n",
    "\n",
    "if 'PORT' not in globals():\n",
    "    # Try to find which port Flask is running on\n",
    "    flask_port = None\n",
    "    for port in range(5000, 5010):\n",
    "        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
    "            if s.connect_ex(('localhost', port)) == 0:\n",
    "                flask_port = port\n",
    "                break\n",
    "    \n",
    "    if flask_port:\n",
    "        PORT = flask_port\n",
    "        print(f\"Detected Flask running on port {PORT}\")\n",
    "    else:\n",
    "        PORT = 5000\n",
    "        print(f\"PORT not defined, using default port {PORT}\")\n",
    "        print(\"âš ï¸  Make sure Flask server cell was run first!\")\n",
    "else:\n",
    "    print(f\"Using PORT={PORT} from Flask server cell\")\n",
    "\n",
    "try:\n",
    "    public_url = ngrok.connect(PORT).public_url\n",
    "except Exception as e:\n",
    "    error_msg = str(e)\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"âŒ NGROK CONNECTION FAILED\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Error: {error_msg}\")\n",
    "    print(f\"Tried to connect to port {PORT}\")\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"ğŸ”§ TO FIX THIS:\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(\"1. Run the cell ABOVE this one (the kill helper cell)\")\n",
    "    print(\"2. OR manually visit: https://dashboard.ngrok.com/agents\")\n",
    "    print(\"3. Stop ALL active ngrok sessions\")\n",
    "    print(\"4. Wait 10 seconds\")\n",
    "    print(\"5. Re-run THIS cell\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(\"\\nğŸ’¡ The free ngrok tier only allows 1 session at a time.\")\n",
    "    print(\"   You must kill the existing session before starting a new one.\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # Don't raise - let user fix it and retry\n",
    "    public_url = None\n",
    "\n",
    "if public_url:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"âœ… ML API READY!\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"\\nPublic URL: {public_url}\")\n",
    "    print(f\"\\nSet in your backend .env:\")\n",
    "    print(f\"TRANSCRIPTION_API_URL={public_url}\")\n",
    "    print(f\"DIARIZATION_API_URL={public_url}\")\n",
    "    print(f\"\\nEndpoints:\")\n",
    "    print(f\"  GET  {public_url}/health     - Check server status\")\n",
    "    print(f\"  POST {public_url}/transcribe - Transcribe audio\")\n",
    "    print(f\"  POST {public_url}/diarize    - Speaker diarization\")\n",
    "else:\n",
    "    print(\"\\nâŒ ngrok tunnel not created. Fix the error above and re-run this cell.\")\n",
    "print(f\"\\nKeep this notebook running!\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  !nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}