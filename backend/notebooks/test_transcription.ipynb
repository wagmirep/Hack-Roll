{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LahStats - MERaLiON Transcription Server\n",
        "\n",
        "Runs MERaLiON as a persistent API server. Your backend calls this endpoint.\n",
        "\n",
        "**Setup:**\n",
        "1. Run all cells\n",
        "2. When prompted, paste your ngrok authtoken (it won't be saved)\n",
        "3. Copy the ngrok URL\n",
        "4. Keep this notebook running during the demo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CUDA available: True\n",
            "GPU: NVIDIA A100-SXM4-40GB\n",
            "Memory: 42.5 GB\n"
          ]
        }
      ],
      "source": [
        "# Check GPU availability\n",
        "import torch\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install -q transformers accelerate librosa soundfile flask pyngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Token set!\n"
          ]
        }
      ],
      "source": [
        "# This notebook is gitignored - safe to paste token here\n",
        "import os\n",
        "os.environ[\"NGROK_AUTHTOKEN\"] = \"31vQrRz6XaMKTpIptTVkuUxhrdW_2McGvyGMxz6KSwgFSZNx8\"  # <-- Paste your token between the quotes\n",
        "print(\"Token set!\" if os.environ.get(\"NGROK_AUTHTOKEN\") else \"Paste your token above!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading processor...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
            "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
            "You are not authenticated with the Hugging Face Hub in this notebook.\n",
            "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b01d8204e72c40fe8d10ae7f38dba3eb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "processor_config.json:   0%|          | 0.00/281 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d9b4761e63d7400fafb8b83466338814",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "processing_meralion2.py: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "A new version of the following files was downloaded from https://huggingface.co/MERaLiON/MERaLiON-2-3B:\n",
            "- processing_meralion2.py\n",
            ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1df78e165d1c4ef0b5b2708ab96ef7e8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "preprocessor_config.json:   0%|          | 0.00/443 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "541b1c2a8f724be1836cc3d4f5e0b169",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b1a001d2ca2d426c97aa06082bba4a74",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "17bbe89a6c084dc190e2ce197e3c613a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/34.4M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "367867846db147d28abb972aad2d3398",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading model (this may take a few minutes)...\n",
            "Using model: MERaLiON/MERaLiON-2-3B\n",
            "GPU Memory: 42.5 GB\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ee378314c5bc4eedbcfb7cad60234cf7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9eaefeb8b65a497ba3b8e6dd01bb14e0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "configuration_meralion2.py: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "A new version of the following files was downloaded from https://huggingface.co/MERaLiON/MERaLiON-2-3B:\n",
            "- configuration_meralion2.py\n",
            ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2b299fa15bc74ea7a86b24388a819c5c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "modeling_meralion2.py: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "A new version of the following files was downloaded from https://huggingface.co/MERaLiON/MERaLiON-2-3B:\n",
            "- modeling_meralion2.py\n",
            ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2e6b562130104a8b8b5f1346d18b67eb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c9b27b82f5f84acbb8ec1cc3756d78fd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a2ecc914980b4b92b95c4d2cf63a3d28",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00002-of-00002.safetensors:   0%|          | 0.00/1.95G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d34b010fae344b479ba6922013201e9b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00001-of-00002.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d02d6ff2d71f46e292df6d5b6e6903a7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a531843e8a114d438665dcb5f42d64e7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/197 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model loaded on GPU (float16)\n",
            "Model loaded on cpu!\n"
          ]
        }
      ],
      "source": [
        "# Load MERaLiON-2-3B-ASR model (smaller, fits on most GPUs)\n",
        "# Use 3B instead of 10B to avoid OOM errors - matches backend service\n",
        "from transformers import AutoProcessor, AutoModelForSpeechSeq2Seq\n",
        "import torch\n",
        "import gc\n",
        "\n",
        "# Clear CUDA cache before loading\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "MODEL_NAME = \"MERaLiON/MERaLiON-2-3B\"  # Changed from 10B to 3B to avoid OOM\n",
        "\n",
        "print(\"Loading processor...\")\n",
        "processor = AutoProcessor.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
        "\n",
        "print(\"Loading model (this may take a few minutes)...\")\n",
        "print(f\"Using model: {MODEL_NAME}\")\n",
        "\n",
        "# Check GPU memory\n",
        "if torch.cuda.is_available():\n",
        "    total_mem = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "    print(f\"GPU Memory: {total_mem:.1f} GB\")\n",
        "    \n",
        "    # Load with float16 for GPU\n",
        "    model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
        "        MODEL_NAME,\n",
        "        torch_dtype=torch.float16,\n",
        "        trust_remote_code=True,\n",
        "        attn_implementation=\"eager\",\n",
        "        low_cpu_mem_usage=True,\n",
        "        device_map=\"auto\",  # Automatic device placement\n",
        "    )\n",
        "    print(f\"Model loaded on GPU (float16)\")\n",
        "else:\n",
        "    # CPU fallback\n",
        "    print(\"No GPU available, loading for CPU...\")\n",
        "    model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
        "        MODEL_NAME,\n",
        "        torch_dtype=torch.float32,\n",
        "        trust_remote_code=True,\n",
        "        attn_implementation=\"eager\",\n",
        "    )\n",
        "    print(\"Model loaded on CPU\")\n",
        "\n",
        "model.eval()  # Set to evaluation mode\n",
        "\n",
        "print(f\"Model loaded on {next(model.parameters()).device}!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Transcription function ready!\n"
          ]
        }
      ],
      "source": [
        "# Transcription function\n",
        "import numpy as np\n",
        "\n",
        "def transcribe(audio_data, sample_rate=16000):\n",
        "    \"\"\"Transcribe audio using MERaLiON.\"\"\"\n",
        "    # Ensure float32 numpy array\n",
        "    if not isinstance(audio_data, np.ndarray):\n",
        "        audio_data = np.array(audio_data)\n",
        "    audio_data = audio_data.astype(np.float32)\n",
        "    \n",
        "    # Chat-style prompt for MERaLiON\n",
        "    prompt_template = \"Instruction: {query} \\nFollow the text instruction based on the following audio: <SpeechHere>\"\n",
        "    transcribe_prompt = \"\"\"Transcribe this Singlish speech using romanized text only. \n",
        "Do NOT use Chinese characters. \n",
        "Write Singlish words in romanized form: walao, shiok, lah, leh, lor, sia, paiseh, sian, etc.\n",
        "Output format: Speaker labels with romanized transcription.\"\"\"\n",
        "    \n",
        "    conversation = [[{\"role\": \"user\", \"content\": prompt_template.format(query=transcribe_prompt)}]]\n",
        "    chat_prompt = processor.tokenizer.apply_chat_template(\n",
        "        conversation=conversation,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True\n",
        "    )\n",
        "    \n",
        "    # Process inputs\n",
        "    inputs = processor(text=chat_prompt, audios=[audio_data])\n",
        "    \n",
        "    # Move to device\n",
        "    device = next(model.parameters()).device\n",
        "    dtype = next(model.parameters()).dtype\n",
        "    \n",
        "    def move_to_device(v):\n",
        "        if not hasattr(v, 'to'):\n",
        "            return v\n",
        "        v = v.to(device)\n",
        "        if v.is_floating_point():\n",
        "            v = v.to(dtype)\n",
        "        return v\n",
        "    \n",
        "    inputs = {k: move_to_device(v) for k, v in inputs.items()}\n",
        "    \n",
        "    # Generate\n",
        "    with torch.no_grad():\n",
        "        generated_ids = model.generate(**inputs, max_new_tokens=256)\n",
        "    \n",
        "    # Decode\n",
        "    transcription = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "    \n",
        "    # Clean up: extract only the model's response (after \"model\\n\")\n",
        "    if \"model\\n\" in transcription:\n",
        "        transcription = transcription.split(\"model\\n\", 1)[-1]\n",
        "    \n",
        "    return transcription.strip()\n",
        "\n",
        "print(\"Transcription function ready!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Post-processing functions ready!\n"
          ]
        }
      ],
      "source": [
        "# Post-processing functions\n",
        "import re\n",
        "from typing import Dict\n",
        "\n",
        "CORRECTIONS = {\n",
        "    'while up': 'walao', 'wah lao': 'walao', 'wa lao': 'walao',\n",
        "    'wah low': 'walao', 'wa low': 'walao', 'while ah': 'walao',\n",
        "    'wah lau': 'walao', 'wa lau': 'walao',\n",
        "    'pie say': 'paiseh', 'pai seh': 'paiseh', 'pie seh': 'paiseh',\n",
        "    'shook': 'shiok', 'she ok': 'shiok',\n",
        "    'see ya': 'sia', 'see ah': 'sia',\n",
        "    'see an': 'sian', 'si an': 'sian',\n",
        "}\n",
        "\n",
        "WORD_CORRECTIONS = {\n",
        "    'la': 'lah', 'low': 'lor', 'loh': 'lor', 'leh': 'lah', 'seh': 'sia',\n",
        "}\n",
        "\n",
        "TARGET_WORDS = [\n",
        "    'walao',  # Keep walao (mild)\n",
        "    'lah', 'lor', 'sia', 'meh', 'leh', 'hor', 'ah',\n",
        "    'can', 'paiseh', 'shiok', 'sian', 'alamak', 'aiyo', 'bodoh', 'kiasu', 'kiasi', 'bojio',\n",
        "]\n",
        "\n",
        "def apply_corrections(text: str) -> str:\n",
        "    if not text:\n",
        "        return text\n",
        "    result = text\n",
        "    for wrong, correct in sorted(CORRECTIONS.items(), key=lambda x: len(x[0]), reverse=True):\n",
        "        pattern = re.compile(re.escape(wrong), re.IGNORECASE)\n",
        "        result = pattern.sub(correct, result)\n",
        "    for wrong, correct in WORD_CORRECTIONS.items():\n",
        "        pattern = re.compile(r'\\b' + re.escape(wrong) + r'\\b', re.IGNORECASE)\n",
        "        result = pattern.sub(correct, result)\n",
        "    return result\n",
        "\n",
        "def count_target_words(text: str) -> Dict[str, int]:\n",
        "    if not text:\n",
        "        return {}\n",
        "    normalized = text.lower()\n",
        "    counts = {}\n",
        "    for word in TARGET_WORDS:\n",
        "        pattern = re.compile(r'(?<![a-zA-Z])' + re.escape(word) + r'(?![a-zA-Z])', re.IGNORECASE)\n",
        "        matches = pattern.findall(normalized)\n",
        "        if matches:\n",
        "            counts[word] = len(matches)\n",
        "    return counts\n",
        "\n",
        "print(\"Post-processing functions ready!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üî• MANUALLY KILL NGROK SESSIONS (Run this if ngrok errors persist)\n",
        "# This cell helps you kill ngrok sessions that pyngrok.kill() can't reach\n",
        "\n",
        "import subprocess\n",
        "import os\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"KILLING ALL NGROK SESSIONS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Method 1: Kill via pyngrok\n",
        "try:\n",
        "    from pyngrok import ngrok\n",
        "    ngrok.kill()\n",
        "    print(\"‚úÖ Killed via pyngrok\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå pyngrok.kill() failed: {e}\")\n",
        "\n",
        "# Method 2: Kill processes (Linux/Mac/Colab)\n",
        "try:\n",
        "    result = subprocess.run(['pkill', '-9', '-f', 'ngrok'], \n",
        "                           capture_output=True, text=True, timeout=5)\n",
        "    if result.returncode == 0:\n",
        "        print(\"‚úÖ Killed ngrok processes via pkill\")\n",
        "    else:\n",
        "        print(\"‚ÑπÔ∏è  No ngrok processes found (or pkill not available)\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ÑπÔ∏è  pkill failed (normal on Windows/Colab): {e}\")\n",
        "\n",
        "# Method 3: Try psutil if available\n",
        "try:\n",
        "    import psutil\n",
        "    killed = 0\n",
        "    for proc in psutil.process_iter(['pid', 'name', 'cmdline']):\n",
        "        try:\n",
        "            cmdline = ' '.join(proc.info['cmdline'] or [])\n",
        "            if 'ngrok' in cmdline.lower():\n",
        "                proc.kill()\n",
        "                killed += 1\n",
        "                print(f\"‚úÖ Killed process PID {proc.info['pid']}\")\n",
        "        except (psutil.NoSuchProcess, psutil.AccessDenied, psutil.ZombieProcess):\n",
        "            pass\n",
        "    if killed == 0:\n",
        "        print(\"‚ÑπÔ∏è  No ngrok processes found via psutil\")\n",
        "except ImportError:\n",
        "    print(\"‚ÑπÔ∏è  psutil not installed (optional)\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ÑπÔ∏è  psutil failed: {e}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"‚ö†Ô∏è  IF ERRORS STILL PERSIST:\")\n",
        "print(\"=\" * 60)\n",
        "print(\"1. Visit: https://dashboard.ngrok.com/agents\")\n",
        "print(\"2. Click 'Stop' on ALL active sessions\")\n",
        "print(\"3. Wait 10 seconds\")\n",
        "print(\"4. Then re-run the ngrok cell below\")\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting Flask server on port 5000...\n",
            "Flask server started!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        }
      ],
      "source": [
        "# Start Flask API server\n",
        "from flask import Flask, request, jsonify\n",
        "import librosa\n",
        "import io\n",
        "import base64\n",
        "import threading\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "@app.route('/health', methods=['GET'])\n",
        "def health():\n",
        "    return jsonify({\"status\": \"ok\", \"model\": \"MERaLiON-2-3B\"})\n",
        "\n",
        "@app.route('/transcribe', methods=['POST'])\n",
        "def transcribe_endpoint():\n",
        "    try:\n",
        "        # Accept audio as base64 or file upload\n",
        "        if request.is_json:\n",
        "            data = request.get_json()\n",
        "            audio_b64 = data.get('audio')\n",
        "            audio_bytes = base64.b64decode(audio_b64)\n",
        "        else:\n",
        "            audio_file = request.files.get('audio')\n",
        "            audio_bytes = audio_file.read()\n",
        "        \n",
        "        # Load audio\n",
        "        audio_data, sr = librosa.load(io.BytesIO(audio_bytes), sr=16000)\n",
        "        \n",
        "        # Transcribe\n",
        "        raw_text = transcribe(audio_data)\n",
        "        \n",
        "        # Post-process\n",
        "        corrected = apply_corrections(raw_text)\n",
        "        counts = count_target_words(corrected)\n",
        "        \n",
        "        return jsonify({\n",
        "            \"raw_transcription\": raw_text,\n",
        "            \"corrected\": corrected,\n",
        "            \"word_counts\": counts,\n",
        "            \"total_singlish_words\": sum(counts.values())\n",
        "        })\n",
        "    except Exception as e:\n",
        "        return jsonify({\"error\": str(e)}), 500\n",
        "\n",
        "# Run Flask in background thread\n",
        "# Try different ports if 5000 is in use\n",
        "import socket\n",
        "\n",
        "def find_free_port(start_port=5000):\n",
        "    for port in range(start_port, start_port + 10):\n",
        "        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
        "            if s.connect_ex(('localhost', port)) != 0:\n",
        "                return port\n",
        "    return None\n",
        "\n",
        "PORT = find_free_port(5000)\n",
        "if PORT is None:\n",
        "    raise RuntimeError(\"Could not find a free port\")\n",
        "\n",
        "print(f\"Starting Flask server on port {PORT}...\")\n",
        "threading.Thread(target=lambda: app.run(host='0.0.0.0', port=PORT, use_reloader=False)).start()\n",
        "print(f\"Flask server started on port {PORT}!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:pyngrok.process.ngrok:t=2026-01-17T18:44:51+0000 lvl=warn msg=\"can't bind default web address, trying alternatives\" obj=web addr=127.0.0.1:4040\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Killed existing ngrok sessions\n",
            "Detected Flask running on port 5000\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ERROR:pyngrok.process.ngrok:t=2026-01-17T18:44:51+0000 lvl=eror msg=\"failed to reconnect session\" obj=tunnels.session err=\"authentication failed: Your account is limited to 1 simultaneous ngrok agent sessions.\\nRun multiple endpoints at the same time from a single agent by defining them in your agent configuration file and running `ngrok start --all`.\\nRead more about the agent configuration file: https://ngrok.com/docs/agent/config/ \\nYou can view your current agent sessions in the dashboard: https://dashboard.ngrok.com/agents. Upgrade to a paid plan to remove this limit:\\nhttps://dashboard.ngrok.com/billing/choose-a-plan\\r\\n\\r\\nERR_NGROK_108\\r\\n\"\n",
            "ERROR:pyngrok.process.ngrok:t=2026-01-17T18:44:51+0000 lvl=eror msg=\"session closing\" obj=tunnels.session err=\"authentication failed: Your account is limited to 1 simultaneous ngrok agent sessions.\\nRun multiple endpoints at the same time from a single agent by defining them in your agent configuration file and running `ngrok start --all`.\\nRead more about the agent configuration file: https://ngrok.com/docs/agent/config/ \\nYou can view your current agent sessions in the dashboard: https://dashboard.ngrok.com/agents. Upgrade to a paid plan to remove this limit:\\nhttps://dashboard.ngrok.com/billing/choose-a-plan\\r\\n\\r\\nERR_NGROK_108\\r\\n\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error connecting ngrok: The ngrok process errored on start: authentication failed: Your account is limited to 1 simultaneous ngrok agent sessions.\\nRun multiple endpoints at the same time from a single agent by defining them in your agent configuration file and running `ngrok start --all`.\\nRead more about the agent configuration file: https://ngrok.com/docs/agent/config/ \\nYou can view your current agent sessions in the dashboard: https://dashboard.ngrok.com/agents. Upgrade to a paid plan to remove this limit:\\nhttps://dashboard.ngrok.com/billing/choose-a-plan\\r\\n\\r\\nERR_NGROK_108\\r\\n.\n",
            "Tried to connect to port 5000\n",
            "Make sure you've killed any existing ngrok sessions\n",
            "Check: https://dashboard.ngrok.com/agents\n"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPyngrokNgrokError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2869562065.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0mpublic_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mngrok\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPORT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpublic_url\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Error connecting ngrok: {e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyngrok/ngrok.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(addr, proto, name, pyngrok_config, **options)\u001b[0m\n\u001b[1;32m    333\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Opening tunnel named: {name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m     \u001b[0mapi_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_ngrok_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyngrok_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_url\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Creating tunnel with options: {options}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyngrok/ngrok.py\u001b[0m in \u001b[0;36mget_ngrok_process\u001b[0;34m(pyngrok_config)\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0minstall_ngrok\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyngrok_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyngrok_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyngrok/process.py\u001b[0m in \u001b[0;36mget_process\u001b[0;34m(pyngrok_config)\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_current_processes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpyngrok_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mngrok_path\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_start_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyngrok_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyngrok/process.py\u001b[0m in \u001b[0;36m_start_process\u001b[0;34m(pyngrok_config)\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mngrok_process\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartup_error\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 447\u001b[0;31m             raise PyngrokNgrokError(f\"The ngrok process errored on start: {ngrok_process.startup_error}.\",\n\u001b[0m\u001b[1;32m    448\u001b[0m                                     \u001b[0mngrok_process\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m                                     ngrok_process.startup_error)\n",
            "\u001b[0;31mPyngrokNgrokError\u001b[0m: The ngrok process errored on start: authentication failed: Your account is limited to 1 simultaneous ngrok agent sessions.\\nRun multiple endpoints at the same time from a single agent by defining them in your agent configuration file and running `ngrok start --all`.\\nRead more about the agent configuration file: https://ngrok.com/docs/agent/config/ \\nYou can view your current agent sessions in the dashboard: https://dashboard.ngrok.com/agents. Upgrade to a paid plan to remove this limit:\\nhttps://dashboard.ngrok.com/billing/choose-a-plan\\r\\n\\r\\nERR_NGROK_108\\r\\n."
          ]
        }
      ],
      "source": [
        "# Expose server via ngrok\n",
        "import os\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# Kill any existing ngrok tunnels first (free tier allows only 1 session)\n",
        "import subprocess\n",
        "import time\n",
        "\n",
        "# Method 1: Use pyngrok kill\n",
        "try:\n",
        "    ngrok.kill()\n",
        "    print(\"Killed existing ngrok sessions (pyngrok)\")\n",
        "    time.sleep(1)  # Wait a bit\n",
        "except Exception as e:\n",
        "    print(f\"pyngrok.kill() failed: {e}\")\n",
        "\n",
        "# Method 2: Kill ngrok processes directly\n",
        "try:\n",
        "    # Find and kill all ngrok processes\n",
        "    result = subprocess.run(['pkill', '-f', 'ngrok'], capture_output=True, text=True)\n",
        "    if result.returncode == 0:\n",
        "        print(\"Killed ngrok processes via pkill\")\n",
        "    time.sleep(1)\n",
        "except Exception as e:\n",
        "    print(f\"pkill failed (might be Windows/Colab): {e}\")\n",
        "    # Try Windows/alternative method\n",
        "    try:\n",
        "        import psutil\n",
        "        for proc in psutil.process_iter(['pid', 'name', 'cmdline']):\n",
        "            try:\n",
        "                if 'ngrok' in ' '.join(proc.info['cmdline'] or []).lower():\n",
        "                    proc.kill()\n",
        "                    print(f\"Killed ngrok process PID {proc.info['pid']}\")\n",
        "            except (psutil.NoSuchProcess, psutil.AccessDenied):\n",
        "                pass\n",
        "        time.sleep(1)\n",
        "    except ImportError:\n",
        "        print(\"psutil not available, skipping process kill\")\n",
        "    except Exception as e:\n",
        "        print(f\"Process kill failed: {e}\")\n",
        "\n",
        "print(\"\\n‚ö†Ô∏è  If errors persist, manually kill ngrok:\")\n",
        "print(\"  1. Visit: https://dashboard.ngrok.com/agents\")\n",
        "print(\"  2. Stop all active sessions\")\n",
        "print(\"  3. Or run: ngrok kill (if ngrok CLI installed)\")\n",
        "print(\"  4. Or run: pkill -f ngrok (Linux/Mac)\")\n",
        "print(\"  5. Or run: taskkill /F /IM ngrok.exe (Windows)\")\n",
        "\n",
        "# Set authtoken from environment or paste directly\n",
        "NGROK_TOKEN = os.environ.get(\"NGROK_AUTHTOKEN\", \"YOUR_TOKEN_HERE\")\n",
        "ngrok.set_auth_token(NGROK_TOKEN)\n",
        "\n",
        "# Start tunnel (use PORT variable from previous cell, or default to 5000)\n",
        "# Check if PORT is defined, otherwise use default or try to detect Flask port\n",
        "import socket\n",
        "\n",
        "if 'PORT' not in globals():\n",
        "    # Try to find which port Flask is running on\n",
        "    flask_port = None\n",
        "    for port in range(5000, 5010):\n",
        "        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
        "            if s.connect_ex(('localhost', port)) == 0:\n",
        "                flask_port = port\n",
        "                break\n",
        "    \n",
        "    if flask_port:\n",
        "        PORT = flask_port\n",
        "        print(f\"Detected Flask running on port {PORT}\")\n",
        "    else:\n",
        "        PORT = 5000\n",
        "        print(f\"PORT not defined, using default port {PORT}\")\n",
        "        print(\"‚ö†Ô∏è  Make sure Flask server cell was run first!\")\n",
        "else:\n",
        "    print(f\"Using PORT={PORT} from Flask server cell\")\n",
        "\n",
        "try:\n",
        "    public_url = ngrok.connect(PORT).public_url\n",
        "except Exception as e:\n",
        "    error_msg = str(e)\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"‚ùå NGROK CONNECTION FAILED\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"Error: {error_msg}\")\n",
        "    print(f\"Tried to connect to port {PORT}\")\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"üîß TO FIX THIS:\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(\"1. Run the cell ABOVE this one (the kill helper cell)\")\n",
        "    print(\"2. OR manually visit: https://dashboard.ngrok.com/agents\")\n",
        "    print(\"3. Stop ALL active ngrok sessions\")\n",
        "    print(\"4. Wait 10 seconds\")\n",
        "    print(\"5. Re-run THIS cell\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(\"\\nüí° The free ngrok tier only allows 1 session at a time.\")\n",
        "    print(\"   You must kill the existing session before starting a new one.\")\n",
        "    print(f\"{'='*60}\\n\")\n",
        "    \n",
        "    # Don't raise - let user fix it and retry\n",
        "    public_url = None\n",
        "\n",
    "if public_url:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"‚úÖ TRANSCRIPTION API READY!\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"\\nPublic URL: {public_url}\")\n",
    "    print(f\"\\nSet in your backend .env:\")\n",
    "    print(f\"TRANSCRIPTION_API_URL={public_url}\")\n",
    "    print(f\"\\nEndpoints:\")\n",
    "    print(f\"  GET  {public_url}/health\")\n",
    "    print(f\"  POST {public_url}/transcribe\")\n",
    "else:\n",
    "    print(\"\\n‚ùå ngrok tunnel not created. Fix the error above and re-run this cell.\")\n",
        "print(f\"\\nKeep this notebook running!\")\n",
        "print(f\"{'='*60}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "  !nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
