{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LahStats - MERaLiON Transcription Server\n",
        "\n",
        "Runs MERaLiON as a persistent API server. Your backend calls this endpoint.\n",
        "\n",
        "**Setup:**\n",
        "1. Run all cells\n",
        "2. When prompted, paste your ngrok authtoken (it won't be saved)\n",
        "3. Copy the ngrok URL\n",
        "4. Keep this notebook running during the demo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CUDA available: True\n",
            "GPU: NVIDIA A100-SXM4-80GB\n",
            "Memory: 85.2 GB\n"
          ]
        }
      ],
      "source": [
        "# Check GPU availability\n",
        "import torch\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install -q transformers accelerate librosa soundfile flask pyngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Token set!\n"
          ]
        }
      ],
      "source": [
        "# This notebook is gitignored - safe to paste token here\n",
        "import os\n",
        "os.environ[\"NGROK_AUTHTOKEN\"] = \"31vQrRz6XaMKTpIptTVkuUxhrdW_2McGvyGMxz6KSwgFSZNx8\"  # <-- Paste your token between the quotes\n",
        "print(\"Token set!\" if os.environ.get(\"NGROK_AUTHTOKEN\") else \"Paste your token above!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading processor...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
            "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
            "You are not authenticated with the Hugging Face Hub in this notebook.\n",
            "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "14d484f431e34786be8dd941750ad6ae",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "processor_config.json:   0%|          | 0.00/281 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "51d7d046ba1345b1a57312b241b6a6d1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "processing_meralion2.py: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "A new version of the following files was downloaded from https://huggingface.co/MERaLiON/MERaLiON-2-3B:\n",
            "- processing_meralion2.py\n",
            ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "01811faf70914874a1c28c5cd1045de9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "preprocessor_config.json:   0%|          | 0.00/443 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "26608b343b7447fa938fa60e2868ff6e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1d54fc391ff24f90bd35119faadccdae",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c15863dc9c7d4fa194776a45ae3d6d4a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/34.4M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "398f782d8673419b8571400c2f1bcd0b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading model (this may take a few minutes)...\n",
            "Using model: MERaLiON/MERaLiON-2-3B\n",
            "GPU Memory: 85.2 GB\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7ecb0706c2104f388c7711ee666fc686",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0f17a2f2f1044eaf88c471a48bdf19a3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "configuration_meralion2.py: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "A new version of the following files was downloaded from https://huggingface.co/MERaLiON/MERaLiON-2-3B:\n",
            "- configuration_meralion2.py\n",
            ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8c5dc23193cc4a92b181b320dcf2fba1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "modeling_meralion2.py: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "A new version of the following files was downloaded from https://huggingface.co/MERaLiON/MERaLiON-2-3B:\n",
            "- modeling_meralion2.py\n",
            ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0fd3bdd4eec449db8b817e231451b747",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c76d4d1bda8a460395d78a7223b8582e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a99f04b7a8b749be8b52af4b41f96020",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00002-of-00002.safetensors:   0%|          | 0.00/1.95G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2e695b531b13473db81bf0009575f6ab",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00001-of-00002.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1509620cfd8643a3814ac9a218a31631",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "97ebc962fb5a46d78bd73a6e2c0d4f3e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/197 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model loaded on GPU (float16)\n",
            "Model loaded on cuda:0!\n"
          ]
        }
      ],
      "source": [
        "# Load MERaLiON-2-3B-ASR model (smaller, fits on most GPUs)\n",
        "# Use 3B instead of 10B to avoid OOM errors - matches backend service\n",
        "from transformers import AutoProcessor, AutoModelForSpeechSeq2Seq\n",
        "import torch\n",
        "import gc\n",
        "\n",
        "# Clear CUDA cache before loading\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "MODEL_NAME = \"MERaLiON/MERaLiON-2-3B\"  # Changed from 10B to 3B to avoid OOM\n",
        "\n",
        "print(\"Loading processor...\")\n",
        "processor = AutoProcessor.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
        "\n",
        "print(\"Loading model (this may take a few minutes)...\")\n",
        "print(f\"Using model: {MODEL_NAME}\")\n",
        "\n",
        "# Check GPU memory\n",
        "if torch.cuda.is_available():\n",
        "    total_mem = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "    print(f\"GPU Memory: {total_mem:.1f} GB\")\n",
        "    \n",
        "    # Load with float16 for GPU\n",
        "    model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
        "        MODEL_NAME,\n",
        "        torch_dtype=torch.float16,\n",
        "        trust_remote_code=True,\n",
        "        attn_implementation=\"eager\",\n",
        "        low_cpu_mem_usage=True,\n",
        "        device_map=\"auto\",  # Automatic device placement\n",
        "    )\n",
        "    print(f\"Model loaded on GPU (float16)\")\n",
        "else:\n",
        "    # CPU fallback\n",
        "    print(\"No GPU available, loading for CPU...\")\n",
        "    model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
        "        MODEL_NAME,\n",
        "        torch_dtype=torch.float32,\n",
        "        trust_remote_code=True,\n",
        "        attn_implementation=\"eager\",\n",
        "    )\n",
        "    print(\"Model loaded on CPU\")\n",
        "\n",
        "model.eval()  # Set to evaluation mode\n",
        "\n",
        "print(f\"Model loaded on {next(model.parameters()).device}!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Transcription function ready!\n"
          ]
        }
      ],
      "source": [
        "# Transcription function\n",
        "import numpy as np\n",
        "\n",
        "def transcribe(audio_data, sample_rate=16000):\n",
        "    \"\"\"Transcribe audio using MERaLiON.\"\"\"\n",
        "    # Ensure float32 numpy array\n",
        "    if not isinstance(audio_data, np.ndarray):\n",
        "        audio_data = np.array(audio_data)\n",
        "    audio_data = audio_data.astype(np.float32)\n",
        "    \n",
        "    # Chat-style prompt for MERaLiON\n",
        "    prompt_template = \"Instruction: {query} \\nFollow the text instruction based on the following audio: <SpeechHere>\"\n",
        "    transcribe_prompt = \"\"\"Transcribe this Singlish speech using romanized text only. \n",
        "Do NOT use Chinese characters. \n",
        "Write Singlish words in romanized form: walao, shiok, lah, leh, lor, sia, paiseh, sian, etc.\n",
        "Output format: Speaker labels with romanized transcription.\"\"\"\n",
        "    \n",
        "    conversation = [[{\"role\": \"user\", \"content\": prompt_template.format(query=transcribe_prompt)}]]\n",
        "    chat_prompt = processor.tokenizer.apply_chat_template(\n",
        "        conversation=conversation,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True\n",
        "    )\n",
        "    \n",
        "    # Process inputs\n",
        "    inputs = processor(text=chat_prompt, audios=[audio_data])\n",
        "    \n",
        "    # Move to device\n",
        "    device = next(model.parameters()).device\n",
        "    dtype = next(model.parameters()).dtype\n",
        "    \n",
        "    def move_to_device(v):\n",
        "        if not hasattr(v, 'to'):\n",
        "            return v\n",
        "        v = v.to(device)\n",
        "        if v.is_floating_point():\n",
        "            v = v.to(dtype)\n",
        "        return v\n",
        "    \n",
        "    inputs = {k: move_to_device(v) for k, v in inputs.items()}\n",
        "    \n",
        "    # Generate\n",
        "    with torch.no_grad():\n",
        "        generated_ids = model.generate(**inputs, max_new_tokens=256)\n",
        "    \n",
        "    # Decode\n",
        "    transcription = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "    \n",
        "    # Clean up: extract only the model's response (after \"model\\n\")\n",
        "    if \"model\\n\" in transcription:\n",
        "        transcription = transcription.split(\"model\\n\", 1)[-1]\n",
        "    \n",
        "    return transcription.strip()\n",
        "\n",
        "print(\"Transcription function ready!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Post-processing functions ready!\n"
          ]
        }
      ],
      "source": [
        "# Post-processing functions\n",
        "import re\n",
        "from typing import Dict\n",
        "\n",
        "CORRECTIONS = {\n",
        "    'while up': 'walao', 'wah lao': 'walao', 'wa lao': 'walao',\n",
        "    'wah low': 'walao', 'wa low': 'walao', 'while ah': 'walao',\n",
        "    'wah lau': 'walao', 'wa lau': 'walao',\n",
        "    'pie say': 'paiseh', 'pai seh': 'paiseh', 'pie seh': 'paiseh',\n",
        "    'shook': 'shiok', 'she ok': 'shiok',\n",
        "    'see ya': 'sia', 'see ah': 'sia',\n",
        "    'see an': 'sian', 'si an': 'sian',\n",
        "}\n",
        "\n",
        "WORD_CORRECTIONS = {\n",
        "    'la': 'lah', 'low': 'lor', 'loh': 'lor', 'leh': 'lah', 'seh': 'sia',\n",
        "}\n",
        "\n",
        "TARGET_WORDS = [\n",
        "    'walao',  # Keep walao (mild)\n",
        "    'lah', 'lor', 'sia', 'meh', 'leh', 'hor', 'ah',\n",
        "    'can', 'paiseh', 'shiok', 'sian', 'alamak', 'aiyo', 'bodoh', 'kiasu', 'kiasi', 'bojio',\n",
        "]\n",
        "\n",
        "def apply_corrections(text: str) -> str:\n",
        "    if not text:\n",
        "        return text\n",
        "    result = text\n",
        "    for wrong, correct in sorted(CORRECTIONS.items(), key=lambda x: len(x[0]), reverse=True):\n",
        "        pattern = re.compile(re.escape(wrong), re.IGNORECASE)\n",
        "        result = pattern.sub(correct, result)\n",
        "    for wrong, correct in WORD_CORRECTIONS.items():\n",
        "        pattern = re.compile(r'\\b' + re.escape(wrong) + r'\\b', re.IGNORECASE)\n",
        "        result = pattern.sub(correct, result)\n",
        "    return result\n",
        "\n",
        "def count_target_words(text: str) -> Dict[str, int]:\n",
        "    if not text:\n",
        "        return {}\n",
        "    normalized = text.lower()\n",
        "    counts = {}\n",
        "    for word in TARGET_WORDS:\n",
        "        pattern = re.compile(r'(?<![a-zA-Z])' + re.escape(word) + r'(?![a-zA-Z])', re.IGNORECASE)\n",
        "        matches = pattern.findall(normalized)\n",
        "        if matches:\n",
        "            counts[word] = len(matches)\n",
        "    return counts\n",
        "\n",
        "print(\"Post-processing functions ready!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "KILLING ALL NGROK SESSIONS\n",
            "============================================================\n",
            "âœ… Killed via pyngrok\n",
            "â„¹ï¸  No ngrok processes found (or pkill not available)\n",
            "â„¹ï¸  No ngrok processes found via psutil\n",
            "\n",
            "============================================================\n",
            "âš ï¸  IF ERRORS STILL PERSIST:\n",
            "============================================================\n",
            "1. Visit: https://dashboard.ngrok.com/agents\n",
            "2. Click 'Stop' on ALL active sessions\n",
            "3. Wait 10 seconds\n",
            "4. Then re-run the ngrok cell below\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# ðŸ”¥ MANUALLY KILL NGROK SESSIONS (Run this if ngrok errors persist)\n",
        "# This cell helps you kill ngrok sessions that pyngrok.kill() can't reach\n",
        "\n",
        "import subprocess\n",
        "import os\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"KILLING ALL NGROK SESSIONS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Method 1: Kill via pyngrok\n",
        "try:\n",
        "    from pyngrok import ngrok\n",
        "    ngrok.kill()\n",
        "    print(\"âœ… Killed via pyngrok\")\n",
        "except Exception as e:\n",
        "    print(f\"âŒ pyngrok.kill() failed: {e}\")\n",
        "\n",
        "# Method 2: Kill processes (Linux/Mac/Colab)\n",
        "try:\n",
        "    result = subprocess.run(['pkill', '-9', '-f', 'ngrok'], \n",
        "                           capture_output=True, text=True, timeout=5)\n",
        "    if result.returncode == 0:\n",
        "        print(\"âœ… Killed ngrok processes via pkill\")\n",
        "    else:\n",
        "        print(\"â„¹ï¸  No ngrok processes found (or pkill not available)\")\n",
        "except Exception as e:\n",
        "    print(f\"â„¹ï¸  pkill failed (normal on Windows/Colab): {e}\")\n",
        "\n",
        "# Method 3: Try psutil if available\n",
        "try:\n",
        "    import psutil\n",
        "    killed = 0\n",
        "    for proc in psutil.process_iter(['pid', 'name', 'cmdline']):\n",
        "        try:\n",
        "            cmdline = ' '.join(proc.info['cmdline'] or [])\n",
        "            if 'ngrok' in cmdline.lower():\n",
        "                proc.kill()\n",
        "                killed += 1\n",
        "                print(f\"âœ… Killed process PID {proc.info['pid']}\")\n",
        "        except (psutil.NoSuchProcess, psutil.AccessDenied, psutil.ZombieProcess):\n",
        "            pass\n",
        "    if killed == 0:\n",
        "        print(\"â„¹ï¸  No ngrok processes found via psutil\")\n",
        "except ImportError:\n",
        "    print(\"â„¹ï¸  psutil not installed (optional)\")\n",
        "except Exception as e:\n",
        "    print(f\"â„¹ï¸  psutil failed: {e}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"âš ï¸  IF ERRORS STILL PERSIST:\")\n",
        "print(\"=\" * 60)\n",
        "print(\"1. Visit: https://dashboard.ngrok.com/agents\")\n",
        "print(\"2. Click 'Stop' on ALL active sessions\")\n",
        "print(\"3. Wait 10 seconds\")\n",
        "print(\"4. Then re-run the ngrok cell below\")\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting Flask server on port 5000...\n",
            "Flask server started on port 5000!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on all addresses (0.0.0.0)\n",
            " * Running on http://127.0.0.1:5000\n",
            " * Running on http://172.28.0.12:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# Start Flask API server\n",
        "from flask import Flask, request, jsonify\n",
        "import librosa\n",
        "import io\n",
        "import base64\n",
        "import threading\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "@app.route('/health', methods=['GET'])\n",
        "def health():\n",
        "    return jsonify({\"status\": \"ok\", \"model\": \"MERaLiON-2-3B\"})\n",
        "\n",
        "@app.route('/transcribe', methods=['POST'])\n",
        "def transcribe_endpoint():\n",
        "    try:\n",
        "        # Accept audio as base64 or file upload\n",
        "        if request.is_json:\n",
        "            data = request.get_json()\n",
        "            audio_b64 = data.get('audio')\n",
        "            audio_bytes = base64.b64decode(audio_b64)\n",
        "        else:\n",
        "            audio_file = request.files.get('audio')\n",
        "            audio_bytes = audio_file.read()\n",
        "        \n",
        "        # Load audio\n",
        "        audio_data, sr = librosa.load(io.BytesIO(audio_bytes), sr=16000)\n",
        "        \n",
        "        # Transcribe\n",
        "        raw_text = transcribe(audio_data)\n",
        "        \n",
        "        # Post-process\n",
        "        corrected = apply_corrections(raw_text)\n",
        "        counts = count_target_words(corrected)\n",
        "        \n",
        "        return jsonify({\n",
        "            \"raw_transcription\": raw_text,\n",
        "            \"corrected\": corrected,\n",
        "            \"word_counts\": counts,\n",
        "            \"total_singlish_words\": sum(counts.values())\n",
        "        })\n",
        "    except Exception as e:\n",
        "        return jsonify({\"error\": str(e)}), 500\n",
        "\n",
        "# Run Flask in background thread\n",
        "# Try different ports if 5000 is in use\n",
        "import socket\n",
        "\n",
        "def find_free_port(start_port=5000):\n",
        "    for port in range(start_port, start_port + 10):\n",
        "        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
        "            if s.connect_ex(('localhost', port)) != 0:\n",
        "                return port\n",
        "    return None\n",
        "\n",
        "PORT = find_free_port(5000)\n",
        "if PORT is None:\n",
        "    raise RuntimeError(\"Could not find a free port\")\n",
        "\n",
        "print(f\"Starting Flask server on port {PORT}...\")\n",
        "threading.Thread(target=lambda: app.run(host='0.0.0.0', port=PORT, use_reloader=False)).start()\n",
        "print(f\"Flask server started on port {PORT}!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Killed existing ngrok sessions (pyngrok)\n",
            "\n",
            "âš ï¸  If errors persist, manually kill ngrok:\n",
            "  1. Visit: https://dashboard.ngrok.com/agents\n",
            "  2. Stop all active sessions\n",
            "  3. Or run: ngrok kill (if ngrok CLI installed)\n",
            "  4. Or run: pkill -f ngrok (Linux/Mac)\n",
            "  5. Or run: taskkill /F /IM ngrok.exe (Windows)\n",
            "Using PORT=5000 from Flask server cell\n",
            "\n",
            "============================================================\n",
            "âœ… TRANSCRIPTION API READY!\n",
            "============================================================\n",
            "\n",
            "Public URL: https://26c654fc02ae.ngrok-free.app\n",
            "\n",
            "Set in your backend .env:\n",
            "TRANSCRIPTION_API_URL=https://26c654fc02ae.ngrok-free.app\n",
            "\n",
            "Endpoints:\n",
            "  GET  https://26c654fc02ae.ngrok-free.app/health\n",
            "  POST https://26c654fc02ae.ngrok-free.app/transcribe\n",
            "\n",
            "Keep this notebook running!\n",
            "============================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [17/Jan/2026 19:42:00] \"POST /transcribe HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [17/Jan/2026 19:43:53] \"POST /transcribe HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [17/Jan/2026 19:43:56] \"POST /transcribe HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [17/Jan/2026 19:45:20] \"POST /transcribe HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [17/Jan/2026 19:45:24] \"POST /transcribe HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [17/Jan/2026 19:48:24] \"POST /transcribe HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [17/Jan/2026 19:48:27] \"POST /transcribe HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [17/Jan/2026 19:48:33] \"POST /transcribe HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [17/Jan/2026 19:50:53] \"POST /transcribe HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [17/Jan/2026 19:50:57] \"POST /transcribe HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [17/Jan/2026 19:53:20] \"POST /transcribe HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [17/Jan/2026 19:53:24] \"POST /transcribe HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [17/Jan/2026 19:53:30] \"POST /transcribe HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [17/Jan/2026 19:53:34] \"POST /transcribe HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [17/Jan/2026 19:53:39] \"POST /transcribe HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [17/Jan/2026 19:53:42] \"POST /transcribe HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [17/Jan/2026 19:56:56] \"POST /transcribe HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [17/Jan/2026 19:56:58] \"POST /transcribe HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [17/Jan/2026 19:57:04] \"POST /transcribe HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [17/Jan/2026 19:57:09] \"POST /transcribe HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [17/Jan/2026 19:57:13] \"POST /transcribe HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [17/Jan/2026 20:01:26] \"POST /transcribe HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [17/Jan/2026 20:01:29] \"POST /transcribe HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [17/Jan/2026 20:01:32] \"POST /transcribe HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [17/Jan/2026 20:06:59] \"POST /transcribe HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [17/Jan/2026 20:07:04] \"POST /transcribe HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [17/Jan/2026 20:12:32] \"POST /transcribe HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [17/Jan/2026 20:12:41] \"POST /transcribe HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [17/Jan/2026 20:12:44] \"POST /transcribe HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [17/Jan/2026 20:12:50] \"POST /transcribe HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [17/Jan/2026 20:12:55] \"POST /transcribe HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [17/Jan/2026 20:12:57] \"POST /transcribe HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [17/Jan/2026 20:14:27] \"POST /transcribe HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [17/Jan/2026 20:14:36] \"POST /transcribe HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [17/Jan/2026 20:14:42] \"POST /transcribe HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [17/Jan/2026 20:15:31] \"POST /transcribe HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [17/Jan/2026 20:15:36] \"POST /transcribe HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [17/Jan/2026 20:15:40] \"POST /transcribe HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [17/Jan/2026 20:15:46] \"POST /transcribe HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [17/Jan/2026 20:15:50] \"POST /transcribe HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [17/Jan/2026 20:18:09] \"POST /transcribe HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [17/Jan/2026 20:18:20] \"POST /transcribe HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [17/Jan/2026 20:18:28] \"POST /transcribe HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [17/Jan/2026 20:22:36] \"POST /transcribe HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [17/Jan/2026 20:22:46] \"POST /transcribe HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [17/Jan/2026 20:22:51] \"POST /transcribe HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [17/Jan/2026 20:25:29] \"POST /transcribe HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [17/Jan/2026 20:25:33] \"POST /transcribe HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [17/Jan/2026 20:25:36] \"POST /transcribe HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [17/Jan/2026 20:28:07] \"POST /transcribe HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [17/Jan/2026 20:28:12] \"POST /transcribe HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [17/Jan/2026 20:31:44] \"POST /transcribe HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [17/Jan/2026 20:31:48] \"POST /transcribe HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [17/Jan/2026 20:35:09] \"POST /transcribe HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [17/Jan/2026 20:35:19] \"POST /transcribe HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [17/Jan/2026 20:35:24] \"POST /transcribe HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [17/Jan/2026 20:39:53] \"POST /transcribe HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [17/Jan/2026 20:39:56] \"POST /transcribe HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [17/Jan/2026 20:40:02] \"POST /transcribe HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [17/Jan/2026 20:40:07] \"POST /transcribe HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [17/Jan/2026 20:40:11] \"POST /transcribe HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [17/Jan/2026 20:40:17] \"POST /transcribe HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [17/Jan/2026 20:40:35] \"POST /transcribe HTTP/1.1\" 200 -\n"
          ]
        }
      ],
      "source": [
        "# Expose server via ngrok\n",
        "import os\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# Kill any existing ngrok tunnels first (free tier allows only 1 session)\n",
        "import subprocess\n",
        "import time\n",
        "\n",
        "# Method 1: Use pyngrok kill\n",
        "try:\n",
        "    ngrok.kill()\n",
        "    print(\"Killed existing ngrok sessions (pyngrok)\")\n",
        "    time.sleep(1)  # Wait a bit\n",
        "except Exception as e:\n",
        "    print(f\"pyngrok.kill() failed: {e}\")\n",
        "\n",
        "# Method 2: Kill ngrok processes directly\n",
        "try:\n",
        "    # Find and kill all ngrok processes\n",
        "    result = subprocess.run(['pkill', '-f', 'ngrok'], capture_output=True, text=True)\n",
        "    if result.returncode == 0:\n",
        "        print(\"Killed ngrok processes via pkill\")\n",
        "    time.sleep(1)\n",
        "except Exception as e:\n",
        "    print(f\"pkill failed (might be Windows/Colab): {e}\")\n",
        "    # Try Windows/alternative method\n",
        "    try:\n",
        "        import psutil\n",
        "        for proc in psutil.process_iter(['pid', 'name', 'cmdline']):\n",
        "            try:\n",
        "                if 'ngrok' in ' '.join(proc.info['cmdline'] or []).lower():\n",
        "                    proc.kill()\n",
        "                    print(f\"Killed ngrok process PID {proc.info['pid']}\")\n",
        "            except (psutil.NoSuchProcess, psutil.AccessDenied):\n",
        "                pass\n",
        "        time.sleep(1)\n",
        "    except ImportError:\n",
        "        print(\"psutil not available, skipping process kill\")\n",
        "    except Exception as e:\n",
        "        print(f\"Process kill failed: {e}\")\n",
        "\n",
        "print(\"\\nâš ï¸  If errors persist, manually kill ngrok:\")\n",
        "print(\"  1. Visit: https://dashboard.ngrok.com/agents\")\n",
        "print(\"  2. Stop all active sessions\")\n",
        "print(\"  3. Or run: ngrok kill (if ngrok CLI installed)\")\n",
        "print(\"  4. Or run: pkill -f ngrok (Linux/Mac)\")\n",
        "print(\"  5. Or run: taskkill /F /IM ngrok.exe (Windows)\")\n",
        "\n",
        "# Set authtoken from environment or paste directly\n",
        "NGROK_TOKEN = os.environ.get(\"NGROK_AUTHTOKEN\", \"YOUR_TOKEN_HERE\")\n",
        "ngrok.set_auth_token(NGROK_TOKEN)\n",
        "\n",
        "# Start tunnel (use PORT variable from previous cell, or default to 5000)\n",
        "# Check if PORT is defined, otherwise use default or try to detect Flask port\n",
        "import socket\n",
        "\n",
        "if 'PORT' not in globals():\n",
        "    # Try to find which port Flask is running on\n",
        "    flask_port = None\n",
        "    for port in range(5000, 5010):\n",
        "        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
        "            if s.connect_ex(('localhost', port)) == 0:\n",
        "                flask_port = port\n",
        "                break\n",
        "    \n",
        "    if flask_port:\n",
        "        PORT = flask_port\n",
        "        print(f\"Detected Flask running on port {PORT}\")\n",
        "    else:\n",
        "        PORT = 5000\n",
        "        print(f\"PORT not defined, using default port {PORT}\")\n",
        "        print(\"âš ï¸  Make sure Flask server cell was run first!\")\n",
        "else:\n",
        "    print(f\"Using PORT={PORT} from Flask server cell\")\n",
        "\n",
        "try:\n",
        "    public_url = ngrok.connect(PORT).public_url\n",
        "except Exception as e:\n",
        "    error_msg = str(e)\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"âŒ NGROK CONNECTION FAILED\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"Error: {error_msg}\")\n",
        "    print(f\"Tried to connect to port {PORT}\")\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"ðŸ”§ TO FIX THIS:\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(\"1. Run the cell ABOVE this one (the kill helper cell)\")\n",
        "    print(\"2. OR manually visit: https://dashboard.ngrok.com/agents\")\n",
        "    print(\"3. Stop ALL active ngrok sessions\")\n",
        "    print(\"4. Wait 10 seconds\")\n",
        "    print(\"5. Re-run THIS cell\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(\"\\nðŸ’¡ The free ngrok tier only allows 1 session at a time.\")\n",
        "    print(\"   You must kill the existing session before starting a new one.\")\n",
        "    print(f\"{'='*60}\\n\")\n",
        "    \n",
        "    # Don't raise - let user fix it and retry\n",
        "    public_url = None\n",
        "\n",
        "if public_url:\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"âœ… TRANSCRIPTION API READY!\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"\\nPublic URL: {public_url}\")\n",
        "    print(f\"\\nSet in your backend .env:\")\n",
        "    print(f\"TRANSCRIPTION_API_URL={public_url}\")\n",
        "    print(f\"\\nEndpoints:\")\n",
        "    print(f\"  GET  {public_url}/health\")\n",
        "    print(f\"  POST {public_url}/transcribe\")\n",
        "else:\n",
        "    print(\"\\nâŒ ngrok tunnel not created. Fix the error above and re-run this cell.\")\n",
        "print(f\"\\nKeep this notebook running!\")\n",
        "print(f\"{'='*60}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sat Jan 17 19:32:15 2026       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA A100-SXM4-80GB          Off |   00000000:00:05.0 Off |                    0 |\n",
            "| N/A   36C    P0             64W /  400W |    7207MiB /  81920MiB |      0%      Default |\n",
            "|                                         |                        |             Disabled |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "  !nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
