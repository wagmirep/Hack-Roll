{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LahStats - MERaLiON Transcription Server\n",
        "\n",
        "Runs MERaLiON as a persistent API server. Your backend calls this endpoint.\n",
        "\n",
        "**Setup:**\n",
        "1. Run all cells\n",
        "2. When prompted, paste your ngrok authtoken (it won't be saved)\n",
        "3. Copy the ngrok URL\n",
        "4. Keep this notebook running during the demo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CUDA available: True\n",
            "GPU: NVIDIA A100-SXM4-80GB\n",
            "Memory: 85.2 GB\n"
          ]
        }
      ],
      "source": [
        "# Check GPU availability\n",
        "import torch\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install -q transformers accelerate librosa soundfile flask pyngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Token set!\n"
          ]
        }
      ],
      "source": [
        "# This notebook is gitignored - safe to paste token here\n",
        "import os\n",
        "os.environ[\"NGROK_AUTHTOKEN\"] = \"31vQrRz6XaMKTpIptTVkuUxhrdW_2McGvyGMxz6KSwgFSZNx8\"  # <-- Paste your token between the quotes\n",
        "print(\"Token set!\" if os.environ.get(\"NGROK_AUTHTOKEN\") else \"Paste your token above!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading processor...\n",
            "Loading model (this may take a few minutes)...\n",
            "Using model: MERaLiON/MERaLiON-2-3B\n",
            "GPU Memory: 85.2 GB\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d82cd92f77b44657a7e7694c223ae500",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model loaded on GPU (float16)\n",
            "Model loaded on cuda:0!\n"
          ]
        }
      ],
      "source": [
        "# Load MERaLiON-2-3B-ASR model (smaller, fits on most GPUs)\n",
        "# Use 3B instead of 10B to avoid OOM errors - matches backend service\n",
        "from transformers import AutoProcessor, AutoModelForSpeechSeq2Seq\n",
        "import torch\n",
        "import gc\n",
        "\n",
        "# Clear CUDA cache before loading\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "MODEL_NAME = \"MERaLiON/MERaLiON-2-3B\"  # Changed from 10B to 3B to avoid OOM\n",
        "\n",
        "print(\"Loading processor...\")\n",
        "processor = AutoProcessor.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
        "\n",
        "print(\"Loading model (this may take a few minutes)...\")\n",
        "print(f\"Using model: {MODEL_NAME}\")\n",
        "\n",
        "# Check GPU memory\n",
        "if torch.cuda.is_available():\n",
        "    total_mem = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "    print(f\"GPU Memory: {total_mem:.1f} GB\")\n",
        "    \n",
        "    # Load with float16 for GPU\n",
        "    model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
        "        MODEL_NAME,\n",
        "        torch_dtype=torch.float16,\n",
        "        trust_remote_code=True,\n",
        "        attn_implementation=\"eager\",\n",
        "        low_cpu_mem_usage=True,\n",
        "        device_map=\"auto\",  # Automatic device placement\n",
        "    )\n",
        "    print(f\"Model loaded on GPU (float16)\")\n",
        "else:\n",
        "    # CPU fallback\n",
        "    print(\"No GPU available, loading for CPU...\")\n",
        "    model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
        "        MODEL_NAME,\n",
        "        torch_dtype=torch.float32,\n",
        "        trust_remote_code=True,\n",
        "        attn_implementation=\"eager\",\n",
        "    )\n",
        "    print(\"Model loaded on CPU\")\n",
        "\n",
        "model.eval()  # Set to evaluation mode\n",
        "\n",
        "print(f\"Model loaded on {next(model.parameters()).device}!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Transcription function ready!\n"
          ]
        }
      ],
      "source": [
        "# Transcription function\n",
        "import numpy as np\n",
        "\n",
        "def transcribe(audio_data, sample_rate=16000):\n",
        "    \"\"\"Transcribe audio using MERaLiON.\"\"\"\n",
        "    # Ensure float32 numpy array\n",
        "    if not isinstance(audio_data, np.ndarray):\n",
        "        audio_data = np.array(audio_data)\n",
        "    audio_data = audio_data.astype(np.float32)\n",
        "    \n",
        "    # Chat-style prompt for MERaLiON\n",
        "    prompt_template = \"Instruction: {query} \\nFollow the text instruction based on the following audio: <SpeechHere>\"\n",
        "    transcribe_prompt = \"\"\"Transcribe this Singlish speech using romanized text only. \n",
        "Do NOT use Chinese characters. \n",
        "Write Singlish words in romanized form: walao, shiok, lah, leh, lor, sia, paiseh, sian, etc.\n",
        "Output format: Speaker labels with romanized transcription.\"\"\"\n",
        "    \n",
        "    conversation = [[{\"role\": \"user\", \"content\": prompt_template.format(query=transcribe_prompt)}]]\n",
        "    chat_prompt = processor.tokenizer.apply_chat_template(\n",
        "        conversation=conversation,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True\n",
        "    )\n",
        "    \n",
        "    # Process inputs\n",
        "    inputs = processor(text=chat_prompt, audios=[audio_data])\n",
        "    \n",
        "    # Move to device\n",
        "    device = next(model.parameters()).device\n",
        "    dtype = next(model.parameters()).dtype\n",
        "    \n",
        "    def move_to_device(v):\n",
        "        if not hasattr(v, 'to'):\n",
        "            return v\n",
        "        v = v.to(device)\n",
        "        if v.is_floating_point():\n",
        "            v = v.to(dtype)\n",
        "        return v\n",
        "    \n",
        "    inputs = {k: move_to_device(v) for k, v in inputs.items()}\n",
        "    \n",
        "    # Generate\n",
        "    with torch.no_grad():\n",
        "        generated_ids = model.generate(**inputs, max_new_tokens=256)\n",
        "    \n",
        "    # Decode\n",
        "    transcription = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "    \n",
        "    # Clean up: extract only the model's response (matches backend _clean_model_output)\n",
        "    if \"model\\n\" in transcription:\n",
        "        transcription = transcription.split(\"model\\n\", 1)[-1]\n",
        "    \n",
        "    # Remove speaker markers like <Speaker1>:, <Speaker2>:, etc.\n",
        "    import re\n",
        "    transcription = re.sub(r'<Speaker\\d+>:\\s*', '', transcription)\n",
        "    # Remove <SpeechHere> tags\n",
        "    transcription = re.sub(r'<SpeechHere>', '', transcription)\n",
        "    # Clean bracketed words: !(walao)! -> walao, (lah) -> lah\n",
        "    transcription = re.sub(r'!\\(([^)]+)\\)!', r'\\1', transcription)\n",
        "    transcription = re.sub(r'\\(([a-zA-Z]+)\\)', r'\\1', transcription)\n",
        "    # Remove filler markers\n",
        "    transcription = re.sub(r'\\(err\\)', '', transcription, flags=re.IGNORECASE)\n",
        "    transcription = re.sub(r'\\(uh\\)', '', transcription, flags=re.IGNORECASE)\n",
        "    transcription = re.sub(r'\\(um\\)', '', transcription, flags=re.IGNORECASE)\n",
        "    # Clean up extra whitespace\n",
        "    transcription = re.sub(r'\\s+', ' ', transcription).strip()\n",
        "    \n",
        "    return transcription\n",
        "\n",
        "print(\"Transcription function ready!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Post-processing functions ready!\n"
          ]
        }
      ],
      "source": [
        "# Post-processing functions\n",
        "import re\n",
        "from typing import Dict\n",
        "\n",
        "CORRECTIONS = {\n",
        "    # Walao variations\n",
        "    'while up': 'walao', 'wah lao eh': 'walao', 'wa lao eh': 'walao',\n",
        "    'wah lao': 'walao', 'wa lao': 'walao', 'wah low': 'walao',\n",
        "    'wa low': 'walao', 'while ah': 'walao', 'wah lau': 'walao',\n",
        "    'wa lau': 'walao', 'wah liao': 'walao', 'wa liao': 'walao',\n",
        "    'while low': 'walao', 'wah lei': 'walao', 'why lao': 'walao',\n",
        "    'why low': 'walao', 'wah la': 'walao',\n",
        "    # Vulgar - cheebai\n",
        "    'cheap buy': 'cheebai', 'chee bye': 'cheebai', 'chi bye': 'cheebai',\n",
        "    'chee bai': 'cheebai', 'chi bai': 'cheebai', 'chee by': 'cheebai',\n",
        "    'chi by': 'cheebai', 'cb': 'cheebai', 'c b': 'cheebai', 'see bee': 'cheebai',\n",
        "    # Vulgar - lanjiao\n",
        "    'lunch hour': 'lanjiao', 'lan jiao': 'lanjiao', 'lan chow': 'lanjiao',\n",
        "    'lan chiao': 'lanjiao', 'lun jiao': 'lanjiao', 'lan chio': 'lanjiao',\n",
        "    'lunchow': 'lanjiao', 'lan jio': 'lanjiao',\n",
        "    # Vulgar - kanina\n",
        "    'can nina': 'kanina', 'kar ni na': 'kanina', 'ka ni na': 'kanina',\n",
        "    'car nina': 'kanina', 'knn': 'kanina', 'k n n': 'kanina',\n",
        "    # Vulgar - nabei\n",
        "    'nah bay': 'nabei', 'na bei': 'nabei', 'nah bei': 'nabei',\n",
        "    'na beh': 'nabei', 'nah beh': 'nabei',\n",
        "    # Paiseh\n",
        "    'pie say': 'paiseh', 'pai seh': 'paiseh', 'pie seh': 'paiseh',\n",
        "    'pai say': 'paiseh', 'pie se': 'paiseh', 'pai se': 'paiseh', 'paise': 'paiseh',\n",
        "    # Shiok\n",
        "    'shook': 'shiok', 'she ok': 'shiok', 'shoe ok': 'shiok', 'shi ok': 'shiok',\n",
        "    # Alamak\n",
        "    'ala mak': 'alamak', 'allah mak': 'alamak', 'a la mak': 'alamak',\n",
        "    'allamak': 'alamak', 'aller mak': 'alamak',\n",
        "    # Aiyo/Aiyah\n",
        "    'ai yo': 'aiyo', 'ai yoh': 'aiyo', 'aiya': 'aiyah', 'ai ya': 'aiyah',\n",
        "    'eye yo': 'aiyo', 'aye yo': 'aiyo', 'ai yah': 'aiyah', 'eye yah': 'aiyah',\n",
        "    # Jialat\n",
        "    'jia lat': 'jialat', 'gia lat': 'jialat', 'jia lut': 'jialat',\n",
        "    'jee ah lat': 'jialat', 'gia lut': 'jialat',\n",
        "    # Bojio\n",
        "    'bo jio': 'bojio', 'boh jio': 'bojio', 'bo gio': 'bojio',\n",
        "    'never jio': 'bojio', 'boh gio': 'bojio',\n",
        "    # Sia/Sian\n",
        "    'see ya': 'sia', 'see ah': 'sia', 'siah': 'sia', 'si ah': 'sia',\n",
        "    'see an': 'sian', 'si an': 'sian', 'see en': 'sian', 'si en': 'sian',\n",
        "    # Other\n",
        "    'kia su': 'kiasu', 'key ah su': 'kiasu', 'kia si': 'kiasi', 'key ah si': 'kiasi',\n",
        "    'boh doh': 'bodoh', 'bo doh': 'bodoh', 'sua ku': 'suaku', 'swah ku': 'suaku',\n",
        "    'le pak': 'lepak', 'lay pak': 'lepak', 'chop': 'chope', 'ma kan': 'makan',\n",
        "    'go stan': 'gostan', 'go stun': 'gostan', 'si bei': 'sibei', 'see bay': 'sibei',\n",
        "    'si bay': 'sibei', 'ah tas': 'atas', 'ar tas': 'atas', 'kay poh': 'kaypoh',\n",
        "    'kae poh': 'kaypoh', 'kaypo': 'kaypoh', 'kpo': 'kaypoh',\n",
        "    'steady pom pi pi': 'steady', 'goon du': 'goondu', 'gun du': 'goondu',\n",
        "}\n",
        "\n",
        "WORD_CORRECTIONS = {\n",
        "    'la': 'lah', 'laa': 'lah', 'laaa': 'lah',\n",
        "    'low': 'lor', 'loh': 'lor',\n",
        "    # 'leh' is a distinct particle - don't convert to 'lah'\n",
        "    'ler': 'lah',\n",
        "    'seh': 'sia',\n",
        "    'arh': 'ah',\n",
        "    'err': 'eh',\n",
        "    'shio': 'shiok',\n",
        "}\n",
        "\n",
        "TARGET_WORDS = [\n",
        "    # Vulgar\n",
        "    'walao', 'cheebai', 'lanjiao', 'kanina', 'nabei',\n",
        "    # Particles\n",
        "    'lah', 'lor', 'sia', 'meh', 'leh', 'hor', 'ah', 'one', 'what', 'lei', 'ma',\n",
        "    # Exclamations\n",
        "    'wah', 'eh', 'huh', 'aiyo', 'aiyah', 'alamak',\n",
        "    # Colloquial\n",
        "    'can', 'cannot', 'paiseh', 'shiok', 'sian', 'bodoh', 'kiasu', 'kiasi',\n",
        "    'bojio', 'suaku', 'lepak', 'blur', 'goondu', 'cheem', 'chim',\n",
        "    # Actions\n",
        "    'chope', 'kena', 'makan', 'tahan', 'gostan', 'cabut', 'sabo', 'arrow',\n",
        "    # Intensifiers\n",
        "    'sibei', 'buay', 'jialat',\n",
        "    # Food/Drink\n",
        "    'kopi', 'teh', 'peng',\n",
        "    # Misc\n",
        "    'atas', 'kaypoh', 'steady', 'power', 'liao',\n",
        "]\n",
        "\n",
        "def apply_corrections(text: str) -> str:\n",
        "    if not text:\n",
        "        return text\n",
        "    result = text\n",
        "    for wrong, correct in sorted(CORRECTIONS.items(), key=lambda x: len(x[0]), reverse=True):\n",
        "        pattern = re.compile(re.escape(wrong), re.IGNORECASE)\n",
        "        result = pattern.sub(correct, result)\n",
        "    for wrong, correct in WORD_CORRECTIONS.items():\n",
        "        pattern = re.compile(r'\\b' + re.escape(wrong) + r'\\b', re.IGNORECASE)\n",
        "        result = pattern.sub(correct, result)\n",
        "    return result\n",
        "\n",
        "def count_target_words(text: str) -> Dict[str, int]:\n",
        "    if not text:\n",
        "        return {}\n",
        "    normalized = text.lower()\n",
        "    counts = {}\n",
        "    for word in TARGET_WORDS:\n",
        "        pattern = re.compile(r'(?<![a-zA-Z])' + re.escape(word) + r'(?![a-zA-Z])', re.IGNORECASE)\n",
        "        matches = pattern.findall(normalized)\n",
        "        if matches:\n",
        "            counts[word] = len(matches)\n",
        "    return counts\n",
        "\n",
        "print(\"Post-processing functions ready!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "KILLING ALL NGROK SESSIONS\n",
            "============================================================\n",
            "âœ… Killed via pyngrok\n",
            "â„¹ï¸  No ngrok processes found (or pkill not available)\n",
            "â„¹ï¸  No ngrok processes found via psutil\n",
            "\n",
            "============================================================\n",
            "âš ï¸  IF ERRORS STILL PERSIST:\n",
            "============================================================\n",
            "1. Visit: https://dashboard.ngrok.com/agents\n",
            "2. Click 'Stop' on ALL active sessions\n",
            "3. Wait 10 seconds\n",
            "4. Then re-run the ngrok cell below\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# ðŸ”¥ MANUALLY KILL NGROK SESSIONS (Run this if ngrok errors persist)\n",
        "# This cell helps you kill ngrok sessions that pyngrok.kill() can't reach\n",
        "\n",
        "import subprocess\n",
        "import os\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"KILLING ALL NGROK SESSIONS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Method 1: Kill via pyngrok\n",
        "try:\n",
        "    from pyngrok import ngrok\n",
        "    ngrok.kill()\n",
        "    print(\"âœ… Killed via pyngrok\")\n",
        "except Exception as e:\n",
        "    print(f\"âŒ pyngrok.kill() failed: {e}\")\n",
        "\n",
        "# Method 2: Kill processes (Linux/Mac/Colab)\n",
        "try:\n",
        "    result = subprocess.run(['pkill', '-9', '-f', 'ngrok'], \n",
        "                           capture_output=True, text=True, timeout=5)\n",
        "    if result.returncode == 0:\n",
        "        print(\"âœ… Killed ngrok processes via pkill\")\n",
        "    else:\n",
        "        print(\"â„¹ï¸  No ngrok processes found (or pkill not available)\")\n",
        "except Exception as e:\n",
        "    print(f\"â„¹ï¸  pkill failed (normal on Windows/Colab): {e}\")\n",
        "\n",
        "# Method 3: Try psutil if available\n",
        "try:\n",
        "    import psutil\n",
        "    killed = 0\n",
        "    for proc in psutil.process_iter(['pid', 'name', 'cmdline']):\n",
        "        try:\n",
        "            cmdline = ' '.join(proc.info['cmdline'] or [])\n",
        "            if 'ngrok' in cmdline.lower():\n",
        "                proc.kill()\n",
        "                killed += 1\n",
        "                print(f\"âœ… Killed process PID {proc.info['pid']}\")\n",
        "        except (psutil.NoSuchProcess, psutil.AccessDenied, psutil.ZombieProcess):\n",
        "            pass\n",
        "    if killed == 0:\n",
        "        print(\"â„¹ï¸  No ngrok processes found via psutil\")\n",
        "except ImportError:\n",
        "    print(\"â„¹ï¸  psutil not installed (optional)\")\n",
        "except Exception as e:\n",
        "    print(f\"â„¹ï¸  psutil failed: {e}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"âš ï¸  IF ERRORS STILL PERSIST:\")\n",
        "print(\"=\" * 60)\n",
        "print(\"1. Visit: https://dashboard.ngrok.com/agents\")\n",
        "print(\"2. Click 'Stop' on ALL active sessions\")\n",
        "print(\"3. Wait 10 seconds\")\n",
        "print(\"4. Then re-run the ngrok cell below\")\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting Flask server on port 5001...\n",
            "Flask server started on port 5001!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on all addresses (0.0.0.0)\n",
            " * Running on http://127.0.0.1:5001\n",
            " * Running on http://172.28.0.12:5001\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# Start Flask API server\n",
        "from flask import Flask, request, jsonify\n",
        "import librosa\n",
        "import io\n",
        "import base64\n",
        "import threading\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "@app.route('/health', methods=['GET'])\n",
        "def health():\n",
        "    return jsonify({\"status\": \"ok\", \"model\": \"MERaLiON-2-3B\"})\n",
        "\n",
        "@app.route('/transcribe', methods=['POST'])\n",
        "def transcribe_endpoint():\n",
        "    try:\n",
        "        # Accept audio as base64 or file upload\n",
        "        if request.is_json:\n",
        "            data = request.get_json()\n",
        "            audio_b64 = data.get('audio')\n",
        "            audio_bytes = base64.b64decode(audio_b64)\n",
        "        else:\n",
        "            audio_file = request.files.get('audio')\n",
        "            audio_bytes = audio_file.read()\n",
        "        \n",
        "        # Load audio\n",
        "        audio_data, sr = librosa.load(io.BytesIO(audio_bytes), sr=16000)\n",
        "        \n",
        "        # Transcribe\n",
        "        raw_text = transcribe(audio_data)\n",
        "        \n",
        "        # Post-process\n",
        "        corrected = apply_corrections(raw_text)\n",
        "        counts = count_target_words(corrected)\n",
        "        \n",
        "        return jsonify({\n",
        "            \"raw_transcription\": raw_text,\n",
        "            \"corrected\": corrected,\n",
        "            \"word_counts\": counts,\n",
        "            \"total_singlish_words\": sum(counts.values())\n",
        "        })\n",
        "    except Exception as e:\n",
        "        return jsonify({\"error\": str(e)}), 500\n",
        "\n",
        "# Run Flask in background thread\n",
        "# Try different ports if 5000 is in use\n",
        "import socket\n",
        "\n",
        "def find_free_port(start_port=5000):\n",
        "    for port in range(start_port, start_port + 10):\n",
        "        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
        "            if s.connect_ex(('localhost', port)) != 0:\n",
        "                return port\n",
        "    return None\n",
        "\n",
        "PORT = find_free_port(5000)\n",
        "if PORT is None:\n",
        "    raise RuntimeError(\"Could not find a free port\")\n",
        "\n",
        "print(f\"Starting Flask server on port {PORT}...\")\n",
        "threading.Thread(target=lambda: app.run(host='0.0.0.0', port=PORT, use_reloader=False)).start()\n",
        "print(f\"Flask server started on port {PORT}!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Killed existing ngrok sessions (pyngrok)\n",
            "\n",
            "âš ï¸  If errors persist, manually kill ngrok:\n",
            "  1. Visit: https://dashboard.ngrok.com/agents\n",
            "  2. Stop all active sessions\n",
            "  3. Or run: ngrok kill (if ngrok CLI installed)\n",
            "  4. Or run: pkill -f ngrok (Linux/Mac)\n",
            "  5. Or run: taskkill /F /IM ngrok.exe (Windows)\n",
            "Using PORT=5001 from Flask server cell\n",
            "\n",
            "============================================================\n",
            "âœ… TRANSCRIPTION API READY!\n",
            "============================================================\n",
            "\n",
            "Public URL: https://93902134a7cd.ngrok-free.app\n",
            "\n",
            "Set in your backend .env:\n",
            "TRANSCRIPTION_API_URL=https://93902134a7cd.ngrok-free.app\n",
            "\n",
            "Endpoints:\n",
            "  GET  https://93902134a7cd.ngrok-free.app/health\n",
            "  POST https://93902134a7cd.ngrok-free.app/transcribe\n",
            "\n",
            "Keep this notebook running!\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# Expose server via ngrok\n",
        "import os\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# Kill any existing ngrok tunnels first (free tier allows only 1 session)\n",
        "import subprocess\n",
        "import time\n",
        "\n",
        "# Method 1: Use pyngrok kill\n",
        "try:\n",
        "    ngrok.kill()\n",
        "    print(\"Killed existing ngrok sessions (pyngrok)\")\n",
        "    time.sleep(1)  # Wait a bit\n",
        "except Exception as e:\n",
        "    print(f\"pyngrok.kill() failed: {e}\")\n",
        "\n",
        "# Method 2: Kill ngrok processes directly\n",
        "try:\n",
        "    # Find and kill all ngrok processes\n",
        "    result = subprocess.run(['pkill', '-f', 'ngrok'], capture_output=True, text=True)\n",
        "    if result.returncode == 0:\n",
        "        print(\"Killed ngrok processes via pkill\")\n",
        "    time.sleep(1)\n",
        "except Exception as e:\n",
        "    print(f\"pkill failed (might be Windows/Colab): {e}\")\n",
        "    # Try Windows/alternative method\n",
        "    try:\n",
        "        import psutil\n",
        "        for proc in psutil.process_iter(['pid', 'name', 'cmdline']):\n",
        "            try:\n",
        "                if 'ngrok' in ' '.join(proc.info['cmdline'] or []).lower():\n",
        "                    proc.kill()\n",
        "                    print(f\"Killed ngrok process PID {proc.info['pid']}\")\n",
        "            except (psutil.NoSuchProcess, psutil.AccessDenied):\n",
        "                pass\n",
        "        time.sleep(1)\n",
        "    except ImportError:\n",
        "        print(\"psutil not available, skipping process kill\")\n",
        "    except Exception as e:\n",
        "        print(f\"Process kill failed: {e}\")\n",
        "\n",
        "print(\"\\nâš ï¸  If errors persist, manually kill ngrok:\")\n",
        "print(\"  1. Visit: https://dashboard.ngrok.com/agents\")\n",
        "print(\"  2. Stop all active sessions\")\n",
        "print(\"  3. Or run: ngrok kill (if ngrok CLI installed)\")\n",
        "print(\"  4. Or run: pkill -f ngrok (Linux/Mac)\")\n",
        "print(\"  5. Or run: taskkill /F /IM ngrok.exe (Windows)\")\n",
        "\n",
        "# Set authtoken from environment or paste directly\n",
        "NGROK_TOKEN = os.environ.get(\"NGROK_AUTHTOKEN\", \"YOUR_TOKEN_HERE\")\n",
        "ngrok.set_auth_token(NGROK_TOKEN)\n",
        "\n",
        "# Start tunnel (use PORT variable from previous cell, or default to 5000)\n",
        "# Check if PORT is defined, otherwise use default or try to detect Flask port\n",
        "import socket\n",
        "\n",
        "if 'PORT' not in globals():\n",
        "    # Try to find which port Flask is running on\n",
        "    flask_port = None\n",
        "    for port in range(5000, 5010):\n",
        "        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
        "            if s.connect_ex(('localhost', port)) == 0:\n",
        "                flask_port = port\n",
        "                break\n",
        "    \n",
        "    if flask_port:\n",
        "        PORT = flask_port\n",
        "        print(f\"Detected Flask running on port {PORT}\")\n",
        "    else:\n",
        "        PORT = 5000\n",
        "        print(f\"PORT not defined, using default port {PORT}\")\n",
        "        print(\"âš ï¸  Make sure Flask server cell was run first!\")\n",
        "else:\n",
        "    print(f\"Using PORT={PORT} from Flask server cell\")\n",
        "\n",
        "try:\n",
        "    public_url = ngrok.connect(PORT).public_url\n",
        "except Exception as e:\n",
        "    error_msg = str(e)\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"âŒ NGROK CONNECTION FAILED\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"Error: {error_msg}\")\n",
        "    print(f\"Tried to connect to port {PORT}\")\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"ðŸ”§ TO FIX THIS:\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(\"1. Run the cell ABOVE this one (the kill helper cell)\")\n",
        "    print(\"2. OR manually visit: https://dashboard.ngrok.com/agents\")\n",
        "    print(\"3. Stop ALL active ngrok sessions\")\n",
        "    print(\"4. Wait 10 seconds\")\n",
        "    print(\"5. Re-run THIS cell\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(\"\\nðŸ’¡ The free ngrok tier only allows 1 session at a time.\")\n",
        "    print(\"   You must kill the existing session before starting a new one.\")\n",
        "    print(f\"{'='*60}\\n\")\n",
        "    \n",
        "    # Don't raise - let user fix it and retry\n",
        "    public_url = None\n",
        "\n",
        "if public_url:\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"âœ… TRANSCRIPTION API READY!\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"\\nPublic URL: {public_url}\")\n",
        "    print(f\"\\nSet in your backend .env:\")\n",
        "    print(f\"TRANSCRIPTION_API_URL={public_url}\")\n",
        "    print(f\"\\nEndpoints:\")\n",
        "    print(f\"  GET  {public_url}/health\")\n",
        "    print(f\"  POST {public_url}/transcribe\")\n",
        "else:\n",
        "    print(\"\\nâŒ ngrok tunnel not created. Fix the error above and re-run this cell.\")\n",
        "print(f\"\\nKeep this notebook running!\")\n",
        "print(f\"{'='*60}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sat Jan 17 21:55:13 2026       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA A100-SXM4-80GB          Off |   00000000:00:05.0 Off |                    0 |\n",
            "| N/A   36C    P0             64W /  400W |   13893MiB /  81920MiB |      0%      Default |\n",
            "|                                         |                        |             Disabled |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [17/Jan/2026 21:59:39] \"POST /transcribe HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [17/Jan/2026 21:59:53] \"POST /transcribe HTTP/1.1\" 200 -\n"
          ]
        }
      ],
      "source": [
        "  !nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
