---
phase: 05-batched-transcription
plan: 02
type: execute
---

<objective>
Use cached transcriptions from background processing for near-instant results after recording ends.

Purpose: Complete the batched transcription feature by integrating cached results into the processor pipeline.
Output: Post-recording processing time reduced from 60-120s to ~5-10s for typical recordings.
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
~/.claude/get-shit-done/templates/summary.md
~/.claude/get-shit-done/references/checkpoints.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/05-batched-transcription/05-CONTEXT.md
@.planning/phases/05-batched-transcription/05-01-SUMMARY.md

# Key source files:
@backend/processor.py
@backend/services/transcription_cache.py
@backend/models.py

**After Plan 05-01:**
- ChunkTranscription model exists
- Chunks transcribed in background as they're uploaded
- Cache lookup functions available

**Current processor.py flow:**
1. Concatenate chunks (0-10%)
2. Diarization (10-40%) - unchanged
3. Transcribe segments sequentially (40-85%) - THIS IS THE BOTTLENECK
4. Save results (85-95%)
5. Generate samples (95-100%)

**Optimization target:**
- Check cache for pre-transcribed text
- Map cached text to speaker segments by timestamp
- Only re-transcribe segments without cache coverage
- Parallelize any remaining transcription work
</context>

<tasks>

<task type="auto">
  <name>Task 1: Refactor processor to use cached transcriptions</name>
  <files>backend/processor.py</files>
  <action>
Modify `transcribe_and_count()` to leverage cached chunk transcriptions:

1. Add imports at top of processor.py:
```python
import asyncio
from services.transcription_cache import get_cached_transcriptions, get_text_for_time_range
```

2. Replace the `transcribe_and_count()` function with this optimized version:

```python
async def transcribe_and_count(
    audio_path: str,
    segments: List[SpeakerSegment],
    db: DBSession,
    session: Session
) -> Dict[str, Dict[str, int]]:
    """
    Transcribe each segment and count target words per speaker.

    OPTIMIZED: Uses cached chunk transcriptions when available,
    falls back to direct transcription for uncached segments.
    Parallelizes remaining transcription work.
    """
    speaker_word_counts: Dict[str, Dict[str, int]] = defaultdict(
        lambda: defaultdict(int)
    )

    total_segments = len(segments)
    session_id = session.id

    # Get cached transcriptions
    cached = get_cached_transcriptions(db, session_id)
    chunks = db.query(AudioChunk).filter(
        AudioChunk.session_id == session_id
    ).order_by(AudioChunk.chunk_number).all()

    logger.info(f"Found {len(cached)} cached chunk transcriptions")

    # Separate segments into cached vs needs-transcription
    cached_segments = []
    uncached_segments = []

    for segment in segments:
        cached_text = get_text_for_time_range(
            cached, chunks, segment.start_time, segment.end_time
        )
        if cached_text:
            cached_segments.append((segment, cached_text))
        else:
            uncached_segments.append(segment)

    logger.info(
        f"Segments: {len(cached_segments)} from cache, "
        f"{len(uncached_segments)} need transcription"
    )

    # Process cached segments (fast - just count words)
    for i, (segment, text) in enumerate(cached_segments):
        corrected = apply_corrections(text)
        word_counts = count_target_words(corrected)

        speaker_id = segment.speaker_id
        for word, count in word_counts.items():
            speaker_word_counts[speaker_id][word] += count

        logger.debug(f"Cached segment ({speaker_id}): {word_counts}")

    # Update progress for cached segments
    cached_progress = len(cached_segments) * 100 // max(total_segments, 1)
    update_progress(db, session, "transcribing", cached_progress)

    # Transcribe uncached segments in parallel
    if uncached_segments:
        async def transcribe_segment(segment: SpeakerSegment) -> Tuple[str, Dict[str, int]]:
            """Transcribe a single segment and return (speaker_id, word_counts)."""
            try:
                segment_bytes = extract_speaker_segment(
                    audio_path, segment.start_time, segment.end_time
                )

                temp_segment = tempfile.NamedTemporaryFile(suffix=".wav", delete=False)
                temp_segment.write(segment_bytes)
                temp_segment.close()

                try:
                    raw_text = transcribe_audio(temp_segment.name)
                    corrected = apply_corrections(raw_text)
                    word_counts = count_target_words(corrected)
                    return (segment.speaker_id, word_counts)
                finally:
                    os.remove(temp_segment.name)

            except Exception as e:
                logger.warning(f"Failed to transcribe segment ({segment.speaker_id}): {e}")
                return (segment.speaker_id, {})

        # Run transcriptions in parallel (limit concurrency to avoid OOM)
        semaphore = asyncio.Semaphore(3)  # Max 3 concurrent transcriptions

        async def limited_transcribe(segment):
            async with semaphore:
                return await transcribe_segment(segment)

        # Execute in parallel
        results = await asyncio.gather(
            *[limited_transcribe(seg) for seg in uncached_segments],
            return_exceptions=True
        )

        # Aggregate results
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                logger.warning(f"Parallel transcription error: {result}")
                continue

            speaker_id, word_counts = result
            for word, count in word_counts.items():
                speaker_word_counts[speaker_id][word] += count

            # Update progress
            progress = (len(cached_segments) + i + 1) * 100 // total_segments
            update_progress(db, session, "transcribing", progress)

    update_progress(db, session, "transcribing", 100)

    return {
        speaker: dict(counts)
        for speaker, counts in speaker_word_counts.items()
    }
```

Key changes:
- Check cache first before transcribing
- Use `get_text_for_time_range()` to map cached chunks to speaker segments
- Process cached segments instantly (just word counting)
- Parallelize uncached segments with `asyncio.gather()`
- Limit concurrency with semaphore to prevent memory issues
- Graceful fallback: if cache miss, transcribe normally

3. Add the Tuple import if not present:
```python
from typing import Dict, List, Optional, Tuple
```
  </action>
  <verify>
- Backend starts without errors
- `python -c "from processor import transcribe_and_count"` works
- No syntax errors in processor.py
  </verify>
  <done>Processor uses cached transcriptions when available, parallelizes remaining work</done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>
Batched transcription system:
- Background transcription of chunks during recording
- Cache-aware processor that skips already-transcribed audio
- Parallel transcription for any remaining segments
  </what-built>
  <how-to-verify>
**Test the full flow:**

1. Start the backend:
   ```bash
   cd backend
   uvicorn main:app --reload --port 8000
   ```

2. Record a session (60-90 seconds):
   - Use the mobile app OR
   - Simulate with curl (upload test audio chunks)

3. During recording, watch logs for:
   ```
   Background transcribing chunk X for session Y
   Cached transcription for chunk X: ...
   ```

4. Stop recording and TIME the processing:
   - Note the timestamp when you stop
   - Watch for "Processing complete for session"
   - Calculate elapsed time

5. Check database for cached transcriptions:
   ```sql
   SELECT chunk_number, length(corrected_text), transcribed_at
   FROM chunk_transcriptions
   WHERE session_id = 'your-session-id'
   ORDER BY chunk_number;
   ```

6. Check processor logs for:
   ```
   Found N cached chunk transcriptions
   Segments: X from cache, Y need transcription
   ```

**Expected results:**
- Background transcription logs appear during recording
- Processing time after stop: 5-15 seconds (vs 60-120s before)
- Most segments come from cache
- Only final chunk + diarization time remains

**If processing still slow:**
- Check if chunks were transcribed (query chunk_transcriptions)
- Check if cache lookup found matches (look for "from cache" in logs)
- Diarization time is unchanged (still ~10-20s for full audio)
  </how-to-verify>
  <resume-signal>Type "approved" if processing time improved significantly, or describe issues to fix</resume-signal>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] Backend starts without errors
- [ ] Background transcription works (logs during recording)
- [ ] Cache entries created in database
- [ ] Processor uses cached transcriptions (logs show "from cache")
- [ ] Processing time significantly reduced (target: 5-15s for 60s recording)
- [ ] Human verification checkpoint passed
</verification>

<success_criteria>

- All tasks completed
- All verification checks pass
- Processing time reduced from ~60-120s to ~5-15s for typical recordings
- No accuracy degradation (same word counts as before)
- Phase 5 complete
</success_criteria>

<output>
After completion, create `.planning/phases/05-batched-transcription/05-02-SUMMARY.md`:

# Phase 5 Plan 02: Fast Post-Processing Summary

**[One-liner describing the outcome]**

## Accomplishments

- Processor refactored to use cached transcriptions
- Parallel transcription for uncached segments
- Processing time reduced from X to Y seconds

## Files Modified

- `backend/processor.py` - Cache-aware transcription with parallel processing

## Decisions Made

[Any architectural decisions]

## Issues Encountered

[Problems and resolutions]

## Next Step

Phase 5 complete. Ready for next milestone or production testing.
</output>
