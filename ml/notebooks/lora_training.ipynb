{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LahStats - LoRA Fine-tuning for Singlish ASR\n",
    "\n",
    "This notebook fine-tunes **MERaLiON-2-10B-ASR** using LoRA adapters for better Singlish word recognition.\n",
    "\n",
    "## What This Does\n",
    "- Loads MERaLiON-2-10B-ASR with 8-bit quantization (fits on Colab T4/A100)\n",
    "- Applies LoRA adapters to attention layers (~1% trainable params)\n",
    "- Trains on your team's Singlish recordings\n",
    "- Evaluates with Word Error Rate (WER)\n",
    "- Saves lightweight adapter (~50MB) for easy deployment\n",
    "\n",
    "## Expected Results\n",
    "- Training time: 2-3 hours on T4, 30-60 min on A100\n",
    "- Expected improvement: +5-15% accuracy on Singlish words\n",
    "- Output: LoRA adapter weights (~50MB)\n",
    "\n",
    "## Data Format\n",
    "Place your data in Google Drive:\n",
    "```\n",
    "lahstats_data/\n",
    "  audio_001.wav\n",
    "  audio_002.wav\n",
    "  transcripts.json  # {\"audio_001.wav\": \"walao eh why like that sia\", ...}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Install Dependencies\n",
    "!pip install -q \\\n",
    "    peft>=0.7.0 \\\n",
    "    transformers>=4.36.0 \\\n",
    "    bitsandbytes>=0.41.0 \\\n",
    "    accelerate>=0.25.0 \\\n",
    "    datasets>=2.14.0 \\\n",
    "    librosa>=0.10.0 \\\n",
    "    soundfile>=0.12.0 \\\n",
    "    evaluate>=0.4.0 \\\n",
    "    jiwer>=3.0.0 \\\n",
    "    torch \\\n",
    "    torchaudio\n",
    "\n",
    "print(\"Installation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Verify GPU\n",
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "    print(f\"GPU: {gpu_name}\")\n",
    "    print(f\"Memory: {gpu_memory:.1f} GB\")\n",
    "else:\n",
    "    print(\"WARNING: No GPU detected!\")\n",
    "    print(\"Go to Runtime > Change runtime type > GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Mount Google Drive & Load Data\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "# === CHANGE THIS PATH TO YOUR DATA FOLDER ===\n",
    "DATA_ROOT = \"/content/drive/MyDrive/lahstats_data\"\n",
    "\n",
    "if os.path.exists(DATA_ROOT):\n",
    "    print(f\"Found data folder: {DATA_ROOT}\")\n",
    "    files = os.listdir(DATA_ROOT)\n",
    "    audio_files = [f for f in files if f.endswith(('.wav', '.mp3', '.m4a'))]\n",
    "    print(f\"Audio files: {len(audio_files)}\")\n",
    "    \n",
    "    # Load transcripts\n",
    "    transcript_path = os.path.join(DATA_ROOT, \"transcripts.json\")\n",
    "    if os.path.exists(transcript_path):\n",
    "        with open(transcript_path) as f:\n",
    "            transcripts = json.load(f)\n",
    "        print(f\"Loaded {len(transcripts)} transcripts\")\n",
    "    else:\n",
    "        print(f\"ERROR: transcripts.json not found at {transcript_path}\")\n",
    "        print(\"Create a JSON file mapping audio filenames to transcriptions\")\n",
    "else:\n",
    "    print(f\"ERROR: Data folder not found: {DATA_ROOT}\")\n",
    "    print(\"Update DATA_ROOT to point to your data folder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Configuration\n",
    "# =============================================================================\n",
    "# CONFIGURATION - Based on research doc recommendations\n",
    "# See: .planning/phases/lora-finetuning/RESEARCH.md\n",
    "# =============================================================================\n",
    "\n",
    "MODEL_NAME = \"MERaLiON/MERaLiON-2-10B-ASR\"\n",
    "\n",
    "# LoRA Settings (from research - r=32, alpha=64 recommended for ASR)\n",
    "LORA_R = 32              # Rank - optimal for ~1000 samples\n",
    "LORA_ALPHA = 64          # Scaling factor - standard is 2x rank\n",
    "LORA_DROPOUT = 0.05      # Light regularization\n",
    "TARGET_MODULES = [\"q_proj\", \"v_proj\"]  # Minimum effective set for ASR\n",
    "\n",
    "# Training Settings\n",
    "BATCH_SIZE = 4           # Reduce to 2 if you get OOM errors\n",
    "GRADIENT_ACCUMULATION = 4  # Effective batch size = 4 * 4 = 16\n",
    "LEARNING_RATE = 1e-4     # Higher than full fine-tuning (which uses 1e-5)\n",
    "NUM_EPOCHS = 3           # More epochs risk overfitting on small datasets\n",
    "WARMUP_STEPS = 50        # ~10% of total steps\n",
    "\n",
    "# Paths\n",
    "OUTPUT_DIR = \"/content/drive/MyDrive/lahstats_lora_checkpoints\"\n",
    "FINAL_ADAPTER_DIR = \"/content/drive/MyDrive/lahstats_lora_adapter\"\n",
    "\n",
    "# Evaluation\n",
    "EVAL_STEPS = 100\n",
    "SAVE_STEPS = 100\n",
    "VAL_SPLIT = 0.15  # 15% for validation\n",
    "\n",
    "print(f\"Config loaded:\")\n",
    "print(f\"  Model: {MODEL_NAME}\")\n",
    "print(f\"  LoRA: r={LORA_R}, alpha={LORA_ALPHA}\")\n",
    "print(f\"  Effective batch size: {BATCH_SIZE * GRADIENT_ACCUMULATION}\")\n",
    "print(f\"  Learning rate: {LEARNING_RATE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Load Model with 8-bit Quantization\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "print(\"Loading MERaLiON with 8-bit quantization...\")\n",
    "print(\"This may take a few minutes on first run (downloading ~20GB)\")\n",
    "\n",
    "# 8-bit quantization config - reduces memory by ~50%\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    ")\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# Load processor\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "print(f\"Model loaded: {model.num_parameters() / 1e9:.1f}B parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Apply LoRA Adapters\n",
    "# Prepare model for k-bit training (handles gradient checkpointing etc)\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model.config.use_cache = False  # Required for gradient checkpointing\n",
    "\n",
    "# Define LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=LORA_R,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    target_modules=TARGET_MODULES,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_2_SEQ_LM\"  # For encoder-decoder ASR models\n",
    ")\n",
    "\n",
    "# Apply LoRA\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Show trainable parameters\n",
    "model.print_trainable_parameters()\n",
    "# Expected output: ~1% trainable parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Prepare Dataset\n",
    "from datasets import Dataset, Audio\n",
    "import os\n",
    "\n",
    "def load_audio_dataset(data_root, transcripts):\n",
    "    \"\"\"\n",
    "    Load audio files and transcripts into a HuggingFace Dataset.\n",
    "    \n",
    "    Expected transcripts format:\n",
    "    {\"audio_001.wav\": \"walao eh the food damn shiok sia\", ...}\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    missing = []\n",
    "    \n",
    "    for audio_file, transcript in transcripts.items():\n",
    "        audio_path = os.path.join(data_root, audio_file)\n",
    "        if os.path.exists(audio_path):\n",
    "            data.append({\n",
    "                \"audio\": audio_path,\n",
    "                \"transcript\": transcript.lower().strip()\n",
    "            })\n",
    "        else:\n",
    "            missing.append(audio_file)\n",
    "    \n",
    "    if missing:\n",
    "        print(f\"Warning: {len(missing)} audio files not found\")\n",
    "        print(f\"  First few: {missing[:5]}\")\n",
    "    \n",
    "    # Create dataset and cast audio column\n",
    "    dataset = Dataset.from_list(data)\n",
    "    dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "# Load dataset\n",
    "full_dataset = load_audio_dataset(DATA_ROOT, transcripts)\n",
    "print(f\"Total samples: {len(full_dataset)}\")\n",
    "\n",
    "# Split into train/validation\n",
    "dataset_split = full_dataset.train_test_split(test_size=VAL_SPLIT, seed=42)\n",
    "train_dataset = dataset_split[\"train\"]\n",
    "val_dataset = dataset_split[\"test\"]\n",
    "\n",
    "print(f\"Train: {len(train_dataset)}, Validation: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Preprocess Audio\n",
    "def prepare_dataset(batch):\n",
    "    \"\"\"\n",
    "    Preprocess audio for MERaLiON:\n",
    "    - Extract log-Mel spectrogram features\n",
    "    - Tokenize transcript as labels\n",
    "    \"\"\"\n",
    "    audio = batch[\"audio\"]\n",
    "    \n",
    "    # Extract features using processor's feature extractor\n",
    "    batch[\"input_features\"] = processor.feature_extractor(\n",
    "        audio[\"array\"],\n",
    "        sampling_rate=audio[\"sampling_rate\"]\n",
    "    ).input_features[0]\n",
    "    \n",
    "    # Tokenize transcript\n",
    "    batch[\"labels\"] = processor.tokenizer(batch[\"transcript\"]).input_ids\n",
    "    \n",
    "    return batch\n",
    "\n",
    "# Apply preprocessing\n",
    "print(\"Preprocessing training data...\")\n",
    "train_dataset = train_dataset.map(\n",
    "    prepare_dataset,\n",
    "    remove_columns=train_dataset.column_names,\n",
    "    num_proc=1  # Use 1 for Colab stability\n",
    ")\n",
    "\n",
    "print(\"Preprocessing validation data...\")\n",
    "val_dataset = val_dataset.map(\n",
    "    prepare_dataset,\n",
    "    remove_columns=val_dataset.column_names,\n",
    "    num_proc=1\n",
    ")\n",
    "\n",
    "print(\"Preprocessing complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Data Collator\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "import torch\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    \"\"\"\n",
    "    Data collator that handles padding for speech-to-text models.\n",
    "    Pads input features and labels to batch max length.\n",
    "    \"\"\"\n",
    "    processor: Any\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # Pad input features (audio spectrograms)\n",
    "        input_features = [{\"input_features\": f[\"input_features\"]} for f in features]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        # Pad labels (text tokens)\n",
    "        label_features = [{\"input_ids\": f[\"labels\"]} for f in features]\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        # Replace padding token id with -100 so it's ignored in loss calculation\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(\n",
    "            labels_batch.attention_mask.ne(1), -100\n",
    "        )\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "        return batch\n",
    "\n",
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)\n",
    "print(\"Data collator ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Evaluation Metrics\n",
    "import evaluate\n",
    "\n",
    "# Load Word Error Rate metric\n",
    "wer_metric = evaluate.load(\"wer\")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    \"\"\"\n",
    "    Compute Word Error Rate (WER) for evaluation.\n",
    "    Lower is better - 0% means perfect transcription.\n",
    "    \"\"\"\n",
    "    pred_ids = pred.predictions\n",
    "    label_ids = pred.label_ids\n",
    "    \n",
    "    # Replace -100 with pad token id for decoding\n",
    "    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "    \n",
    "    # Decode predictions and labels\n",
    "    pred_str = processor.tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = processor.tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "    \n",
    "    # Compute WER (multiply by 100 for percentage)\n",
    "    wer = 100 * wer_metric.compute(predictions=pred_str, references=label_str)\n",
    "    \n",
    "    return {\"wer\": wer}\n",
    "\n",
    "print(\"Metrics ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Training Setup\n",
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "import os\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    \n",
    "    # Batch size\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION,\n",
    "    \n",
    "    # Learning rate\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    warmup_steps=WARMUP_STEPS,\n",
    "    \n",
    "    # Training duration\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    \n",
    "    # Memory optimization\n",
    "    fp16=True,  # Mixed precision training\n",
    "    gradient_checkpointing=True,  # Trade compute for memory\n",
    "    \n",
    "    # Evaluation\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=EVAL_STEPS,\n",
    "    \n",
    "    # Checkpointing\n",
    "    save_steps=SAVE_STEPS,\n",
    "    save_total_limit=3,  # Keep only last 3 checkpoints\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"wer\",\n",
    "    greater_is_better=False,  # Lower WER is better\n",
    "    \n",
    "    # Generation settings for evaluation\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=225,\n",
    "    \n",
    "    # Logging\n",
    "    logging_steps=25,\n",
    "    report_to=\"tensorboard\",\n",
    "    \n",
    "    # Other\n",
    "    remove_unused_columns=False,\n",
    "    label_names=[\"labels\"],\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=processor.feature_extractor,\n",
    ")\n",
    "\n",
    "print(\"Trainer ready!\")\n",
    "print(f\"Checkpoints will be saved to: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: TRAIN!\n",
    "print(\"=\" * 60)\n",
    "print(\"STARTING TRAINING\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "print(f\"Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"Effective batch size: {BATCH_SIZE * GRADIENT_ACCUMULATION}\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\")\n",
    "\n",
    "# Train!\n",
    "trainer.train()\n",
    "\n",
    "print(\"\")\n",
    "print(\"=\" * 60)\n",
    "print(\"TRAINING COMPLETE!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13: Save Final Adapter\n",
    "import os\n",
    "\n",
    "os.makedirs(FINAL_ADAPTER_DIR, exist_ok=True)\n",
    "\n",
    "# Save LoRA adapter weights (~50MB)\n",
    "model.save_pretrained(FINAL_ADAPTER_DIR)\n",
    "\n",
    "# Save processor for inference\n",
    "processor.save_pretrained(FINAL_ADAPTER_DIR)\n",
    "\n",
    "print(f\"\")\n",
    "print(f\"LoRA adapter saved to: {FINAL_ADAPTER_DIR}\")\n",
    "print(f\"\")\n",
    "print(f\"Contents:\")\n",
    "for f in os.listdir(FINAL_ADAPTER_DIR):\n",
    "    size = os.path.getsize(os.path.join(FINAL_ADAPTER_DIR, f)) / 1024 / 1024\n",
    "    print(f\"  {f}: {size:.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 14: Test the Fine-tuned Model\n",
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "def transcribe_test(audio_path):\n",
    "    \"\"\"Test transcription with the fine-tuned model.\"\"\"\n",
    "    # Load audio\n",
    "    audio, sr = librosa.load(audio_path, sr=16000)\n",
    "    \n",
    "    # Prepare input\n",
    "    inputs = processor(\n",
    "        audio,\n",
    "        sampling_rate=16000,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(**inputs, max_new_tokens=256)\n",
    "    \n",
    "    # Decode\n",
    "    transcription = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    return transcription\n",
    "\n",
    "# Test with first audio file\n",
    "test_file = list(transcripts.keys())[0]\n",
    "test_path = os.path.join(DATA_ROOT, test_file)\n",
    "\n",
    "if os.path.exists(test_path):\n",
    "    print(f\"Testing with: {test_file}\")\n",
    "    print(f\"Expected: {transcripts[test_file]}\")\n",
    "    print(f\"Got: {transcribe_test(test_path)}\")\n",
    "else:\n",
    "    print(f\"Test file not found: {test_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Use Your Trained Adapter\n",
    "\n",
    "```python\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor\n",
    "from peft import PeftModel\n",
    "\n",
    "# Load base model\n",
    "base_model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    \"MERaLiON/MERaLiON-2-10B-ASR\",\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# Load your LoRA adapter\n",
    "model = PeftModel.from_pretrained(base_model, \"./lahstats_lora_adapter\")\n",
    "processor = AutoProcessor.from_pretrained(\"./lahstats_lora_adapter\")\n",
    "\n",
    "# Optional: Merge adapter into base model for faster inference\n",
    "# model = model.merge_and_unload()\n",
    "\n",
    "# Use for transcription!\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
